{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5FyEkTbAdUD"
      },
      "source": [
        "# **Challenge 1**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKIuPv4lGEEJ",
        "outputId": "00954085-2bfe-4c3c-fa15-c068cfd38809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hanzla-khalid98/challenge_AN2DL_2025.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WenSZOLfGfV8",
        "outputId": "c7449495-c72b-4579-a884-789a66b0e87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'challenge_AN2DL_2025'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 112 (delta 52), reused 67 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (112/112), 17.12 MiB | 15.81 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n",
            "Filtering content: 100% (4/4), 186.05 MiB | 58.75 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/hanzla-khalid98/challenge_AN2DL_2025/src')"
      ],
      "metadata": {
        "id": "yOjoYihlGsQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfehjCy896Fd"
      },
      "source": [
        "## ‚öôÔ∏è **Libraries Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x47Uv8R9Akcl",
        "outputId": "95c49df4-2bb7-4745-809d-1862cb777fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4g715TtSytf"
      },
      "outputs": [],
      "source": [
        "data = torch.load(\"/content/drive/MyDrive/dataset.pt\")\n",
        "\n",
        "train_ds = TensorDataset(data[\"X_train\"], data[\"y_train\"])\n",
        "val_ds   = TensorDataset(data[\"X_val\"], data[\"y_val\"])\n",
        "test_ds  = TensorDataset(data[\"X_test\"], data[\"y_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjhjKSW3WysO"
      },
      "outputs": [],
      "source": [
        "# Define the batch size, which is the number of samples in each batch\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIkKaZJkW0z_"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZR5CG9qW2AI"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with different settings for each phase\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPpKL3_NW2dq"
      },
      "outputs": [],
      "source": [
        "# Get one batch from the training data loader\n",
        "for xb, yb in train_loader:\n",
        "    input_shape = xb.shape[1:]\n",
        "    num_classes = len(np.unique(yb))\n",
        "    break # Stop after getting one batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfejlDqBYOOM"
      },
      "source": [
        "## üõ†Ô∏è **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VraH-QTDxHWD"
      },
      "outputs": [],
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogXWKej_fl6p"
      },
      "outputs": [],
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic RNN classifier (RNN, LSTM, GRU).\n",
        "    Uses the last hidden state for classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Map string name to PyTorch RNN class\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "\n",
        "        if rnn_type not in rnn_map:\n",
        "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
        "\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        # Dropout is only applied between layers (if num_layers > 1)\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        # Create the recurrent layer\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        # Calculate input size for the final classifier\n",
        "        if self.bidirectional:\n",
        "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
        "        else:\n",
        "            classifier_input_size = hidden_size\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        rnn_out, hidden = self.rnn(x)\n",
        "\n",
        "        # LSTM returns (h_n, c_n), we only need h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "\n",
        "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
        "            # Final shape: (batch_size, hidden_size * 2)\n",
        "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
        "        else:\n",
        "            # Take the last layer's hidden state\n",
        "            # Final shape: (batch_size, hidden_size)\n",
        "            hidden_to_classify = hidden[-1]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(hidden_to_classify)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BywFq9R9ma8K"
      },
      "source": [
        "## üßÆ **Network and Training Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxtAqB60mcAd"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 500\n",
        "PATIENCE = 50\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = 128        # Neurons per layer\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.3         # Dropout probability\n",
        "L1_LAMBDA = 1.0           # L1 penalty\n",
        "L2_LAMBDA = 0.0            # L2 penalty"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_frequency = np.array([0.77, 0.14, 0.09])\n",
        "\n",
        "class_weights = torch.tensor(1/class_frequency, dtype=torch.float32).to(device)\n",
        "\n",
        "# Set up loss function and optimizer with class weights and label smoothing\n",
        "criterion = nn.CrossEntropyLoss(class_weights, label_smoothing=0.1)"
      ],
      "metadata": {
        "id": "qoKLJ0Ykwnj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agr5qySeoI5B"
      },
      "source": [
        "## üß† **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxUcNEn7oJ9-"
      },
      "outputs": [],
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3TgsfOzoMeW"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR7Oa-m4oOm2"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujrKlgKoQ3s"
      },
      "outputs": [],
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRCpArKIoS2C"
      },
      "outputs": [],
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_loss, val_f1 = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=False,\n",
        "    rnn_type='LSTM'\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "hparams = {\n",
        "    \"input_size\": input_shape[-1], # Pass the number of features\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"num_layers\": HIDDEN_LAYERS,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"dropout_rate\": DROPOUT_RATE,\n",
        "    \"bidirectional\": False,\n",
        "    \"rnn_type\": 'LSTM'\n",
        "}\n",
        "\n",
        "torch.save(hparams, \"models/bi_lstm_hparams.pt\")\n",
        "\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "experiment_name = \"rnn\"\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "writer.add_graph(rnn_model, x)\n",
        "\n",
        "# Define optimizer with L2 regularization\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J9LfTM_D4S9",
        "outputId": "c02f2b3d-80ba-4b16-d72d-b0410a658f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "rnn (LSTM)                [[-1, 50, 128], [2, -1, 128]] 217,600        \n",
            "classifier (Linear)       [-1, 3]                      387            \n",
            "===============================================================================\n",
            "Total params: 217,987\n",
            "Trainable params: 217,987\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llBX--t7NAE3"
      },
      "source": [
        "### **Long Short-Term Memory (LSTM)**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1AHDpl1vMWow9xUhP4C7nZLJjk_kNos_I\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "astW9HWsOik3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f398ae3-c0b9-4801-82e4-4d066d9ba587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "rnn (LSTM)                [[-1, 50, 128], [2, -1, 128]] 217,600        \n",
            "classifier (Linear)       [-1, 3]                      387            \n",
            "===============================================================================\n",
            "Total params: 217,987\n",
            "Trainable params: 217,987\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=False,\n",
        "    rnn_type='LSTM'\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "experiment_name = \"lstm\"\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "writer.add_graph(rnn_model, x)\n",
        "\n",
        "# Define optimizer with L2 regularization\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEuRDgEEOiiQ",
        "outputId": "c06d2334-34f9-45db-dab5-dd6f1a03500f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 500 epochs...\n",
            "Epoch   1/500 | Train: Loss=1.2166, F1 Score=0.3644 | Val: Loss=1.2369, F1 Score=0.6935\n",
            "Epoch   2/500 | Train: Loss=1.1893, F1 Score=0.5945 | Val: Loss=1.2741, F1 Score=0.3971\n",
            "Epoch   3/500 | Train: Loss=1.1601, F1 Score=0.5725 | Val: Loss=1.2239, F1 Score=0.7464\n",
            "Epoch   4/500 | Train: Loss=1.1807, F1 Score=0.4779 | Val: Loss=1.2645, F1 Score=0.3658\n",
            "Epoch   5/500 | Train: Loss=1.1423, F1 Score=0.6136 | Val: Loss=1.2187, F1 Score=0.6719\n",
            "Epoch   6/500 | Train: Loss=1.1753, F1 Score=0.5583 | Val: Loss=1.2914, F1 Score=0.3963\n",
            "Epoch   7/500 | Train: Loss=1.1825, F1 Score=0.4374 | Val: Loss=1.2738, F1 Score=0.4971\n",
            "Epoch   8/500 | Train: Loss=1.1264, F1 Score=0.6766 | Val: Loss=1.2255, F1 Score=0.7182\n",
            "Epoch   9/500 | Train: Loss=1.0769, F1 Score=0.7815 | Val: Loss=1.2158, F1 Score=0.7034\n",
            "Epoch  10/500 | Train: Loss=1.0653, F1 Score=0.7879 | Val: Loss=1.1811, F1 Score=0.7871\n",
            "Epoch  11/500 | Train: Loss=1.0549, F1 Score=0.7796 | Val: Loss=1.2162, F1 Score=0.6421\n",
            "Epoch  12/500 | Train: Loss=1.0583, F1 Score=0.7706 | Val: Loss=1.2174, F1 Score=0.7542\n",
            "Epoch  13/500 | Train: Loss=1.0242, F1 Score=0.7964 | Val: Loss=1.1802, F1 Score=0.7928\n",
            "Epoch  14/500 | Train: Loss=1.0631, F1 Score=0.7227 | Val: Loss=1.1721, F1 Score=0.8011\n",
            "Epoch  15/500 | Train: Loss=0.9616, F1 Score=0.8453 | Val: Loss=1.1595, F1 Score=0.7694\n",
            "Epoch  16/500 | Train: Loss=0.9837, F1 Score=0.8064 | Val: Loss=1.1186, F1 Score=0.8084\n",
            "Epoch  17/500 | Train: Loss=0.9730, F1 Score=0.8110 | Val: Loss=1.1347, F1 Score=0.8216\n",
            "Epoch  18/500 | Train: Loss=0.9762, F1 Score=0.8292 | Val: Loss=1.1483, F1 Score=0.7903\n",
            "Epoch  19/500 | Train: Loss=0.9602, F1 Score=0.8331 | Val: Loss=1.1024, F1 Score=0.8490\n",
            "Epoch  20/500 | Train: Loss=0.9437, F1 Score=0.8374 | Val: Loss=1.1450, F1 Score=0.8061\n",
            "Epoch  21/500 | Train: Loss=0.9112, F1 Score=0.8200 | Val: Loss=1.1766, F1 Score=0.7899\n",
            "Epoch  22/500 | Train: Loss=0.9165, F1 Score=0.8261 | Val: Loss=1.0673, F1 Score=0.7186\n",
            "Epoch  23/500 | Train: Loss=0.8816, F1 Score=0.8268 | Val: Loss=1.1361, F1 Score=0.8331\n",
            "Epoch  24/500 | Train: Loss=0.9258, F1 Score=0.8088 | Val: Loss=1.0410, F1 Score=0.7696\n",
            "Epoch  25/500 | Train: Loss=0.8970, F1 Score=0.8247 | Val: Loss=1.0955, F1 Score=0.8068\n",
            "Epoch  26/500 | Train: Loss=0.9373, F1 Score=0.8097 | Val: Loss=1.1126, F1 Score=0.8250\n",
            "Epoch  27/500 | Train: Loss=0.8824, F1 Score=0.8097 | Val: Loss=1.0401, F1 Score=0.7030\n",
            "Epoch  28/500 | Train: Loss=0.8799, F1 Score=0.8297 | Val: Loss=1.0211, F1 Score=0.7616\n",
            "Epoch  29/500 | Train: Loss=0.8173, F1 Score=0.8470 | Val: Loss=1.0325, F1 Score=0.8389\n",
            "Epoch  30/500 | Train: Loss=0.8345, F1 Score=0.8558 | Val: Loss=1.0928, F1 Score=0.8478\n",
            "Epoch  31/500 | Train: Loss=0.8113, F1 Score=0.8465 | Val: Loss=1.0354, F1 Score=0.6963\n",
            "Epoch  32/500 | Train: Loss=0.7974, F1 Score=0.8408 | Val: Loss=1.0671, F1 Score=0.8438\n",
            "Epoch  33/500 | Train: Loss=0.8172, F1 Score=0.8609 | Val: Loss=1.0592, F1 Score=0.8071\n",
            "Epoch  34/500 | Train: Loss=0.7961, F1 Score=0.8520 | Val: Loss=1.0307, F1 Score=0.7239\n",
            "Epoch  35/500 | Train: Loss=0.7554, F1 Score=0.8578 | Val: Loss=1.0382, F1 Score=0.8267\n",
            "Epoch  36/500 | Train: Loss=0.7228, F1 Score=0.8771 | Val: Loss=1.0844, F1 Score=0.8629\n",
            "Epoch  37/500 | Train: Loss=0.7379, F1 Score=0.8869 | Val: Loss=1.0119, F1 Score=0.8731\n",
            "Epoch  38/500 | Train: Loss=0.7560, F1 Score=0.8837 | Val: Loss=1.0150, F1 Score=0.8638\n",
            "Epoch  39/500 | Train: Loss=0.7133, F1 Score=0.8895 | Val: Loss=1.0366, F1 Score=0.8726\n",
            "Epoch  40/500 | Train: Loss=0.7457, F1 Score=0.8825 | Val: Loss=0.9828, F1 Score=0.8836\n",
            "Epoch  41/500 | Train: Loss=0.7037, F1 Score=0.8871 | Val: Loss=0.9861, F1 Score=0.8599\n",
            "Epoch  42/500 | Train: Loss=0.7108, F1 Score=0.8883 | Val: Loss=1.0136, F1 Score=0.8859\n",
            "Epoch  43/500 | Train: Loss=0.7095, F1 Score=0.8817 | Val: Loss=0.9920, F1 Score=0.8696\n",
            "Epoch  44/500 | Train: Loss=0.6864, F1 Score=0.8942 | Val: Loss=0.9786, F1 Score=0.8827\n",
            "Epoch  45/500 | Train: Loss=0.7335, F1 Score=0.8966 | Val: Loss=0.9847, F1 Score=0.8775\n",
            "Epoch  46/500 | Train: Loss=0.6893, F1 Score=0.8983 | Val: Loss=0.9648, F1 Score=0.8770\n",
            "Epoch  47/500 | Train: Loss=0.7139, F1 Score=0.8975 | Val: Loss=0.9749, F1 Score=0.8356\n",
            "Epoch  48/500 | Train: Loss=0.7455, F1 Score=0.8884 | Val: Loss=1.0017, F1 Score=0.8877\n",
            "Epoch  49/500 | Train: Loss=0.7149, F1 Score=0.8901 | Val: Loss=0.9576, F1 Score=0.8862\n",
            "Epoch  50/500 | Train: Loss=0.7364, F1 Score=0.8916 | Val: Loss=0.9569, F1 Score=0.8529\n",
            "Epoch  51/500 | Train: Loss=0.7162, F1 Score=0.8897 | Val: Loss=0.9631, F1 Score=0.8762\n",
            "Epoch  52/500 | Train: Loss=0.6804, F1 Score=0.9040 | Val: Loss=0.9250, F1 Score=0.8540\n",
            "Epoch  53/500 | Train: Loss=0.6687, F1 Score=0.9067 | Val: Loss=1.0929, F1 Score=0.8579\n",
            "Epoch  54/500 | Train: Loss=0.6656, F1 Score=0.9065 | Val: Loss=0.9551, F1 Score=0.9098\n",
            "Epoch  55/500 | Train: Loss=0.7027, F1 Score=0.8909 | Val: Loss=0.9461, F1 Score=0.8970\n",
            "Epoch  56/500 | Train: Loss=0.6530, F1 Score=0.9135 | Val: Loss=0.9255, F1 Score=0.8993\n",
            "Epoch  57/500 | Train: Loss=0.6399, F1 Score=0.9244 | Val: Loss=0.9236, F1 Score=0.8976\n",
            "Epoch  58/500 | Train: Loss=0.6977, F1 Score=0.9116 | Val: Loss=1.1744, F1 Score=0.8001\n",
            "Epoch  59/500 | Train: Loss=0.7588, F1 Score=0.8832 | Val: Loss=0.9906, F1 Score=0.8885\n",
            "Epoch  60/500 | Train: Loss=0.6549, F1 Score=0.9239 | Val: Loss=0.9917, F1 Score=0.8696\n",
            "Epoch  61/500 | Train: Loss=0.6799, F1 Score=0.9052 | Val: Loss=0.9565, F1 Score=0.8554\n",
            "Epoch  62/500 | Train: Loss=0.6474, F1 Score=0.9260 | Val: Loss=0.9240, F1 Score=0.8932\n",
            "Epoch  63/500 | Train: Loss=0.6409, F1 Score=0.9232 | Val: Loss=0.9226, F1 Score=0.8823\n",
            "Epoch  64/500 | Train: Loss=0.6188, F1 Score=0.9344 | Val: Loss=0.9460, F1 Score=0.8911\n",
            "Epoch  65/500 | Train: Loss=0.6646, F1 Score=0.9265 | Val: Loss=0.9415, F1 Score=0.9034\n",
            "Epoch  66/500 | Train: Loss=0.6359, F1 Score=0.9335 | Val: Loss=0.9329, F1 Score=0.9091\n",
            "Epoch  67/500 | Train: Loss=0.6200, F1 Score=0.9394 | Val: Loss=0.9246, F1 Score=0.8963\n",
            "Epoch  68/500 | Train: Loss=0.6200, F1 Score=0.9402 | Val: Loss=0.9228, F1 Score=0.8980\n",
            "Epoch  69/500 | Train: Loss=0.6336, F1 Score=0.9315 | Val: Loss=0.9465, F1 Score=0.8983\n",
            "Epoch  70/500 | Train: Loss=0.6217, F1 Score=0.9449 | Val: Loss=0.9285, F1 Score=0.8951\n",
            "Epoch  71/500 | Train: Loss=0.6247, F1 Score=0.9422 | Val: Loss=0.9628, F1 Score=0.8953\n",
            "Epoch  72/500 | Train: Loss=0.6330, F1 Score=0.9298 | Val: Loss=0.9475, F1 Score=0.8500\n",
            "Epoch  73/500 | Train: Loss=0.6124, F1 Score=0.9419 | Val: Loss=0.9397, F1 Score=0.8925\n",
            "Epoch  74/500 | Train: Loss=0.6115, F1 Score=0.9455 | Val: Loss=0.9641, F1 Score=0.8726\n",
            "Epoch  75/500 | Train: Loss=0.6383, F1 Score=0.9435 | Val: Loss=0.9602, F1 Score=0.8872\n",
            "Epoch  76/500 | Train: Loss=0.6260, F1 Score=0.9459 | Val: Loss=0.9679, F1 Score=0.8591\n",
            "Epoch  77/500 | Train: Loss=0.6084, F1 Score=0.9492 | Val: Loss=0.9012, F1 Score=0.8930\n",
            "Epoch  78/500 | Train: Loss=0.6154, F1 Score=0.9532 | Val: Loss=0.8923, F1 Score=0.8892\n",
            "Epoch  79/500 | Train: Loss=0.6037, F1 Score=0.9546 | Val: Loss=1.0124, F1 Score=0.8547\n",
            "Epoch  80/500 | Train: Loss=0.6073, F1 Score=0.9493 | Val: Loss=0.9438, F1 Score=0.8793\n",
            "Epoch  81/500 | Train: Loss=0.6254, F1 Score=0.9423 | Val: Loss=0.9321, F1 Score=0.9006\n",
            "Epoch  82/500 | Train: Loss=0.6543, F1 Score=0.9436 | Val: Loss=1.0595, F1 Score=0.8425\n",
            "Epoch  83/500 | Train: Loss=0.7882, F1 Score=0.8867 | Val: Loss=0.9556, F1 Score=0.8859\n",
            "Epoch  84/500 | Train: Loss=0.6342, F1 Score=0.9375 | Val: Loss=0.9021, F1 Score=0.9070\n",
            "Epoch  85/500 | Train: Loss=0.6037, F1 Score=0.9543 | Val: Loss=0.9082, F1 Score=0.9075\n",
            "Epoch  86/500 | Train: Loss=0.5906, F1 Score=0.9645 | Val: Loss=0.9444, F1 Score=0.8699\n",
            "Epoch  87/500 | Train: Loss=0.5935, F1 Score=0.9547 | Val: Loss=0.9249, F1 Score=0.8981\n",
            "Epoch  88/500 | Train: Loss=0.6268, F1 Score=0.9454 | Val: Loss=0.9289, F1 Score=0.9059\n",
            "Epoch  89/500 | Train: Loss=0.5934, F1 Score=0.9530 | Val: Loss=0.9824, F1 Score=0.8758\n",
            "Epoch  90/500 | Train: Loss=0.6120, F1 Score=0.9445 | Val: Loss=0.9598, F1 Score=0.8759\n",
            "Epoch  91/500 | Train: Loss=0.5860, F1 Score=0.9634 | Val: Loss=0.9035, F1 Score=0.9042\n",
            "Epoch  92/500 | Train: Loss=0.5893, F1 Score=0.9613 | Val: Loss=0.9016, F1 Score=0.9011\n",
            "Epoch  93/500 | Train: Loss=0.5958, F1 Score=0.9582 | Val: Loss=0.9762, F1 Score=0.8832\n",
            "Epoch  94/500 | Train: Loss=0.6546, F1 Score=0.9323 | Val: Loss=0.9426, F1 Score=0.8882\n",
            "Epoch  95/500 | Train: Loss=0.5946, F1 Score=0.9588 | Val: Loss=0.8871, F1 Score=0.9102\n",
            "Epoch  96/500 | Train: Loss=0.5788, F1 Score=0.9700 | Val: Loss=0.9113, F1 Score=0.9008\n",
            "Epoch  97/500 | Train: Loss=0.5801, F1 Score=0.9699 | Val: Loss=0.9088, F1 Score=0.9082\n",
            "Epoch  98/500 | Train: Loss=0.6063, F1 Score=0.9595 | Val: Loss=0.9474, F1 Score=0.9042\n",
            "Epoch  99/500 | Train: Loss=0.5951, F1 Score=0.9578 | Val: Loss=0.9271, F1 Score=0.8796\n",
            "Epoch 100/500 | Train: Loss=0.6062, F1 Score=0.9493 | Val: Loss=0.9131, F1 Score=0.8752\n",
            "Epoch 101/500 | Train: Loss=0.6259, F1 Score=0.9468 | Val: Loss=0.9202, F1 Score=0.8988\n",
            "Epoch 102/500 | Train: Loss=0.5916, F1 Score=0.9649 | Val: Loss=0.8911, F1 Score=0.9178\n",
            "Epoch 103/500 | Train: Loss=0.5876, F1 Score=0.9659 | Val: Loss=0.9237, F1 Score=0.8902\n",
            "Epoch 104/500 | Train: Loss=0.5965, F1 Score=0.9562 | Val: Loss=0.9205, F1 Score=0.8992\n",
            "Epoch 105/500 | Train: Loss=0.5793, F1 Score=0.9683 | Val: Loss=0.9568, F1 Score=0.9050\n",
            "Epoch 106/500 | Train: Loss=0.6790, F1 Score=0.9257 | Val: Loss=1.0423, F1 Score=0.8672\n",
            "Epoch 107/500 | Train: Loss=0.6663, F1 Score=0.9368 | Val: Loss=1.0317, F1 Score=0.8542\n",
            "Epoch 108/500 | Train: Loss=0.6065, F1 Score=0.9550 | Val: Loss=0.9530, F1 Score=0.8964\n",
            "Epoch 109/500 | Train: Loss=0.6042, F1 Score=0.9555 | Val: Loss=0.9743, F1 Score=0.8790\n",
            "Epoch 110/500 | Train: Loss=0.6346, F1 Score=0.9450 | Val: Loss=1.0108, F1 Score=0.8650\n",
            "Epoch 111/500 | Train: Loss=0.5756, F1 Score=0.9714 | Val: Loss=0.9779, F1 Score=0.8681\n",
            "Epoch 112/500 | Train: Loss=0.5765, F1 Score=0.9694 | Val: Loss=0.9798, F1 Score=0.8882\n",
            "Epoch 113/500 | Train: Loss=0.5822, F1 Score=0.9685 | Val: Loss=0.9293, F1 Score=0.9112\n",
            "Epoch 114/500 | Train: Loss=0.5929, F1 Score=0.9649 | Val: Loss=0.8969, F1 Score=0.9214\n",
            "Epoch 115/500 | Train: Loss=0.5806, F1 Score=0.9701 | Val: Loss=0.9398, F1 Score=0.8903\n",
            "Epoch 116/500 | Train: Loss=0.6267, F1 Score=0.9524 | Val: Loss=0.9666, F1 Score=0.8771\n",
            "Epoch 117/500 | Train: Loss=0.5816, F1 Score=0.9677 | Val: Loss=0.9112, F1 Score=0.9109\n",
            "Epoch 118/500 | Train: Loss=0.5708, F1 Score=0.9761 | Val: Loss=0.8897, F1 Score=0.9255\n",
            "Epoch 119/500 | Train: Loss=0.5687, F1 Score=0.9778 | Val: Loss=0.9960, F1 Score=0.8860\n",
            "Epoch 120/500 | Train: Loss=0.6067, F1 Score=0.9560 | Val: Loss=0.9290, F1 Score=0.9112\n",
            "Epoch 121/500 | Train: Loss=0.5844, F1 Score=0.9691 | Val: Loss=0.8939, F1 Score=0.9070\n",
            "Epoch 122/500 | Train: Loss=0.5923, F1 Score=0.9596 | Val: Loss=0.9050, F1 Score=0.8976\n",
            "Epoch 123/500 | Train: Loss=0.5849, F1 Score=0.9582 | Val: Loss=0.9119, F1 Score=0.9002\n",
            "Epoch 124/500 | Train: Loss=0.5834, F1 Score=0.9628 | Val: Loss=0.9148, F1 Score=0.9202\n",
            "Epoch 125/500 | Train: Loss=0.5802, F1 Score=0.9698 | Val: Loss=0.9246, F1 Score=0.8889\n",
            "Epoch 126/500 | Train: Loss=0.5726, F1 Score=0.9778 | Val: Loss=0.9364, F1 Score=0.8856\n",
            "Epoch 127/500 | Train: Loss=0.5690, F1 Score=0.9744 | Val: Loss=0.9784, F1 Score=0.8666\n",
            "Epoch 128/500 | Train: Loss=0.5959, F1 Score=0.9585 | Val: Loss=0.9631, F1 Score=0.8810\n",
            "Epoch 129/500 | Train: Loss=0.6401, F1 Score=0.9400 | Val: Loss=0.9803, F1 Score=0.8690\n",
            "Epoch 130/500 | Train: Loss=0.6241, F1 Score=0.9548 | Val: Loss=0.9233, F1 Score=0.9114\n",
            "Epoch 131/500 | Train: Loss=0.5848, F1 Score=0.9674 | Val: Loss=0.9528, F1 Score=0.8902\n",
            "Epoch 132/500 | Train: Loss=0.5805, F1 Score=0.9724 | Val: Loss=0.8953, F1 Score=0.9186\n",
            "Epoch 133/500 | Train: Loss=0.5715, F1 Score=0.9703 | Val: Loss=0.8815, F1 Score=0.9269\n",
            "Epoch 134/500 | Train: Loss=0.5675, F1 Score=0.9774 | Val: Loss=0.9249, F1 Score=0.8996\n",
            "Epoch 135/500 | Train: Loss=0.5686, F1 Score=0.9771 | Val: Loss=0.9404, F1 Score=0.8856\n",
            "Epoch 136/500 | Train: Loss=0.5784, F1 Score=0.9722 | Val: Loss=0.9964, F1 Score=0.8840\n",
            "Epoch 137/500 | Train: Loss=0.6053, F1 Score=0.9600 | Val: Loss=0.9450, F1 Score=0.8957\n",
            "Epoch 138/500 | Train: Loss=0.5778, F1 Score=0.9701 | Val: Loss=0.9250, F1 Score=0.9049\n",
            "Epoch 139/500 | Train: Loss=0.5984, F1 Score=0.9595 | Val: Loss=0.9313, F1 Score=0.9043\n",
            "Epoch 140/500 | Train: Loss=0.5861, F1 Score=0.9664 | Val: Loss=0.9520, F1 Score=0.8915\n",
            "Epoch 141/500 | Train: Loss=0.5750, F1 Score=0.9710 | Val: Loss=0.9246, F1 Score=0.8813\n",
            "Epoch 142/500 | Train: Loss=0.5641, F1 Score=0.9761 | Val: Loss=0.8570, F1 Score=0.9254\n",
            "Epoch 143/500 | Train: Loss=0.5625, F1 Score=0.9804 | Val: Loss=0.9052, F1 Score=0.9036\n",
            "Epoch 144/500 | Train: Loss=0.5628, F1 Score=0.9817 | Val: Loss=0.9301, F1 Score=0.8830\n",
            "Epoch 145/500 | Train: Loss=0.6304, F1 Score=0.9524 | Val: Loss=0.9760, F1 Score=0.8725\n",
            "Epoch 146/500 | Train: Loss=0.6931, F1 Score=0.9287 | Val: Loss=0.9800, F1 Score=0.8940\n",
            "Epoch 147/500 | Train: Loss=0.5963, F1 Score=0.9619 | Val: Loss=0.9271, F1 Score=0.8879\n",
            "Epoch 148/500 | Train: Loss=0.5712, F1 Score=0.9767 | Val: Loss=1.0064, F1 Score=0.8683\n",
            "Epoch 149/500 | Train: Loss=0.5715, F1 Score=0.9728 | Val: Loss=0.9057, F1 Score=0.8991\n",
            "Epoch 150/500 | Train: Loss=0.5657, F1 Score=0.9771 | Val: Loss=0.9407, F1 Score=0.8786\n",
            "Epoch 151/500 | Train: Loss=0.5746, F1 Score=0.9708 | Val: Loss=0.9209, F1 Score=0.9024\n",
            "Epoch 152/500 | Train: Loss=0.5696, F1 Score=0.9790 | Val: Loss=0.9005, F1 Score=0.9023\n",
            "Epoch 153/500 | Train: Loss=0.5638, F1 Score=0.9817 | Val: Loss=1.0184, F1 Score=0.8526\n",
            "Epoch 154/500 | Train: Loss=0.5629, F1 Score=0.9803 | Val: Loss=0.9556, F1 Score=0.8882\n",
            "Epoch 155/500 | Train: Loss=0.5612, F1 Score=0.9802 | Val: Loss=1.0142, F1 Score=0.8401\n",
            "Epoch 156/500 | Train: Loss=0.5813, F1 Score=0.9669 | Val: Loss=0.9291, F1 Score=0.9095\n",
            "Epoch 157/500 | Train: Loss=0.5658, F1 Score=0.9805 | Val: Loss=0.9584, F1 Score=0.8798\n",
            "Epoch 158/500 | Train: Loss=0.5644, F1 Score=0.9836 | Val: Loss=0.9474, F1 Score=0.8946\n",
            "Epoch 159/500 | Train: Loss=0.5639, F1 Score=0.9799 | Val: Loss=0.9382, F1 Score=0.8897\n",
            "Epoch 160/500 | Train: Loss=0.5621, F1 Score=0.9828 | Val: Loss=0.9560, F1 Score=0.8912\n",
            "Epoch 161/500 | Train: Loss=0.5863, F1 Score=0.9673 | Val: Loss=0.9743, F1 Score=0.8846\n",
            "Epoch 162/500 | Train: Loss=0.5940, F1 Score=0.9649 | Val: Loss=0.9803, F1 Score=0.8862\n",
            "Epoch 163/500 | Train: Loss=0.5913, F1 Score=0.9670 | Val: Loss=0.9577, F1 Score=0.8833\n",
            "Epoch 164/500 | Train: Loss=0.6279, F1 Score=0.9492 | Val: Loss=0.9779, F1 Score=0.8763\n",
            "Epoch 165/500 | Train: Loss=0.5739, F1 Score=0.9739 | Val: Loss=0.9397, F1 Score=0.8985\n",
            "Epoch 166/500 | Train: Loss=0.5646, F1 Score=0.9798 | Val: Loss=1.0250, F1 Score=0.8552\n",
            "Epoch 167/500 | Train: Loss=0.5784, F1 Score=0.9753 | Val: Loss=0.9099, F1 Score=0.9186\n",
            "Epoch 168/500 | Train: Loss=0.5723, F1 Score=0.9687 | Val: Loss=0.8950, F1 Score=0.9104\n",
            "Epoch 169/500 | Train: Loss=0.5604, F1 Score=0.9802 | Val: Loss=0.9339, F1 Score=0.9113\n",
            "Epoch 170/500 | Train: Loss=0.5822, F1 Score=0.9712 | Val: Loss=0.9495, F1 Score=0.8723\n",
            "Epoch 171/500 | Train: Loss=0.5732, F1 Score=0.9738 | Val: Loss=0.9638, F1 Score=0.8861\n",
            "Epoch 172/500 | Train: Loss=0.5655, F1 Score=0.9790 | Val: Loss=0.9912, F1 Score=0.8752\n",
            "Epoch 173/500 | Train: Loss=0.5592, F1 Score=0.9858 | Val: Loss=0.9658, F1 Score=0.8922\n",
            "Epoch 174/500 | Train: Loss=0.5639, F1 Score=0.9820 | Val: Loss=0.9364, F1 Score=0.9116\n",
            "Epoch 175/500 | Train: Loss=0.5638, F1 Score=0.9797 | Val: Loss=1.0327, F1 Score=0.8713\n",
            "Epoch 176/500 | Train: Loss=0.5586, F1 Score=0.9842 | Val: Loss=0.9890, F1 Score=0.8902\n",
            "Epoch 177/500 | Train: Loss=0.5773, F1 Score=0.9706 | Val: Loss=1.0269, F1 Score=0.8636\n",
            "Epoch 178/500 | Train: Loss=0.6241, F1 Score=0.9517 | Val: Loss=0.9434, F1 Score=0.9037\n",
            "Epoch 179/500 | Train: Loss=0.5619, F1 Score=0.9826 | Val: Loss=0.9225, F1 Score=0.8992\n",
            "Epoch 180/500 | Train: Loss=0.5628, F1 Score=0.9815 | Val: Loss=0.9612, F1 Score=0.9032\n",
            "Epoch 181/500 | Train: Loss=0.5599, F1 Score=0.9846 | Val: Loss=0.9096, F1 Score=0.9197\n",
            "Epoch 182/500 | Train: Loss=0.5566, F1 Score=0.9882 | Val: Loss=0.9572, F1 Score=0.8855\n",
            "Epoch 183/500 | Train: Loss=0.5629, F1 Score=0.9804 | Val: Loss=0.9500, F1 Score=0.8915\n",
            "Early stopping triggered after 183 epochs.\n",
            "Best model restored from epoch 133 with val_f1 0.9269\n",
            "CPU times: user 4min 37s, sys: 30.4 s, total: 5min 7s\n",
            "Wall time: 5min 8s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Train model and track training history\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=1,\n",
        "    experiment_name=\"lstm\",\n",
        "    patience=PATIENCE\n",
        "    )\n",
        "\n",
        "# Update best model if current performance is superior\n",
        "if training_history['val_f1'][-1] > best_performance:\n",
        "    best_model = rnn_model\n",
        "    best_performance = training_history['val_f1'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "Mt_ZjMnjOifb",
        "outputId": "16ad9353-d15b-4bf2-ed5a-44e5dd62e03b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2825036448.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot of training and validation loss on the first axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#ff7f0e'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#ff7f0e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Plot Hitory\n",
        "# Create a figure with two side-by-side subplots (two columns)\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "# Plot of training and validation loss on the first axis\n",
        "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot of training and validation accuracy on the second axis\n",
        "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
        "ax2.set_title('F1 Score')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxaww_Kne7m6"
      },
      "outputs": [],
      "source": [
        "# @title Plot Confusion Matrix\n",
        "# Collect predictions and ground truth labels\n",
        "val_preds, val_targets = [], []\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        logits = rnn_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store batch results\n",
        "        val_preds.append(preds)\n",
        "        val_targets.append(yb.numpy())\n",
        "\n",
        "# Combine all batches into single arrays\n",
        "val_preds = np.concatenate(val_preds)\n",
        "val_targets = np.concatenate(val_targets)\n",
        "\n",
        "# Calculate overall validation metrics\n",
        "val_acc = accuracy_score(val_targets, val_preds)\n",
        "val_prec = precision_score(val_targets, val_preds, average='weighted')\n",
        "val_rec = recall_score(val_targets, val_preds, average='weighted')\n",
        "val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
        "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
        "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
        "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
        "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for detailed error analysis\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "# Create numeric labels for heatmap annotation\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm, annot=labels, fmt='',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix ‚Äî Validation Set')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bPCQoeXPjQY"
      },
      "source": [
        "### **Bidirectional Long Short-Term Memory (BiLSTM)**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-W_s-cH_9wv-rmfKaa5giXzIXpdRFUpf\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwQ_6RuoOick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80ebf8e-73ce-4e40-b905-25a0b65a45ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape                 Param #           \n",
            "===============================================================================\n",
            "rnn (LSTM)                [[-1, 50, 128], [4, -1, 64]] 152,064        \n",
            "classifier (Linear)       [-1, 3]                      387            \n",
            "===============================================================================\n",
            "Total params: 152,451\n",
            "Trainable params: 152,451\n",
            "Non-trainable params: 0\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=True,\n",
        "    rnn_type='LSTM'\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "hparams = {\n",
        "    \"input_size\": input_shape[-1], # Pass the number of features\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"num_layers\": HIDDEN_LAYERS,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"dropout_rate\": DROPOUT_RATE,\n",
        "    \"bidirectional\": True,\n",
        "    \"rnn_type\": 'LSTM'\n",
        "}\n",
        "\n",
        "torch.save(hparams, \"models/bi_lstm_hparams.pt\")\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "experiment_name = \"bi_lstm\"\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "writer.add_graph(rnn_model, x)\n",
        "\n",
        "# Define optimizer with L2 regularization\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yNxK3ctOiZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5829548c-9d07-4feb-afc5-8ae17a4a6044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 500 epochs...\n",
            "Epoch   1/500 | Train: Loss=0.6777, F1 Score=0.6819 | Val: Loss=0.6476, F1 Score=0.6798\n",
            "Epoch   2/500 | Train: Loss=0.5451, F1 Score=0.7608 | Val: Loss=0.5523, F1 Score=0.7653\n",
            "Epoch   3/500 | Train: Loss=0.4335, F1 Score=0.8166 | Val: Loss=0.6658, F1 Score=0.7688\n",
            "Epoch   4/500 | Train: Loss=0.3840, F1 Score=0.8524 | Val: Loss=0.5552, F1 Score=0.8186\n",
            "Epoch   5/500 | Train: Loss=0.3582, F1 Score=0.8670 | Val: Loss=0.5705, F1 Score=0.8080\n",
            "Epoch   6/500 | Train: Loss=0.3774, F1 Score=0.8572 | Val: Loss=0.6235, F1 Score=0.7336\n",
            "Epoch   7/500 | Train: Loss=0.3415, F1 Score=0.8706 | Val: Loss=0.5651, F1 Score=0.7933\n",
            "Epoch   8/500 | Train: Loss=0.2924, F1 Score=0.8977 | Val: Loss=0.5201, F1 Score=0.8344\n",
            "Epoch   9/500 | Train: Loss=0.2649, F1 Score=0.9045 | Val: Loss=0.4942, F1 Score=0.8205\n",
            "Epoch  10/500 | Train: Loss=0.2608, F1 Score=0.9080 | Val: Loss=0.4738, F1 Score=0.8336\n",
            "Epoch  11/500 | Train: Loss=0.2604, F1 Score=0.9057 | Val: Loss=0.6705, F1 Score=0.7417\n",
            "Epoch  12/500 | Train: Loss=0.2513, F1 Score=0.9144 | Val: Loss=0.3686, F1 Score=0.8592\n",
            "Epoch  13/500 | Train: Loss=0.2264, F1 Score=0.9197 | Val: Loss=0.3732, F1 Score=0.8607\n",
            "Epoch  14/500 | Train: Loss=0.2111, F1 Score=0.9235 | Val: Loss=0.3493, F1 Score=0.8756\n",
            "Epoch  15/500 | Train: Loss=0.3082, F1 Score=0.8936 | Val: Loss=0.4206, F1 Score=0.8418\n",
            "Epoch  16/500 | Train: Loss=0.2072, F1 Score=0.9280 | Val: Loss=0.3601, F1 Score=0.8724\n",
            "Epoch  17/500 | Train: Loss=0.1953, F1 Score=0.9315 | Val: Loss=0.4115, F1 Score=0.8570\n",
            "Epoch  18/500 | Train: Loss=0.1948, F1 Score=0.9310 | Val: Loss=0.3817, F1 Score=0.8814\n",
            "Epoch  19/500 | Train: Loss=0.1979, F1 Score=0.9287 | Val: Loss=0.3400, F1 Score=0.8786\n",
            "Epoch  20/500 | Train: Loss=0.1673, F1 Score=0.9374 | Val: Loss=0.3741, F1 Score=0.8878\n",
            "Epoch  21/500 | Train: Loss=0.1457, F1 Score=0.9459 | Val: Loss=0.4502, F1 Score=0.8623\n",
            "Epoch  22/500 | Train: Loss=0.1523, F1 Score=0.9450 | Val: Loss=0.4366, F1 Score=0.8744\n",
            "Epoch  23/500 | Train: Loss=0.1808, F1 Score=0.9344 | Val: Loss=0.4135, F1 Score=0.8692\n",
            "Epoch  24/500 | Train: Loss=0.1685, F1 Score=0.9436 | Val: Loss=0.4526, F1 Score=0.8699\n",
            "Epoch  25/500 | Train: Loss=0.1333, F1 Score=0.9508 | Val: Loss=0.3218, F1 Score=0.8892\n",
            "Epoch  26/500 | Train: Loss=0.1260, F1 Score=0.9546 | Val: Loss=0.3921, F1 Score=0.8826\n",
            "Epoch  27/500 | Train: Loss=0.1101, F1 Score=0.9596 | Val: Loss=0.3579, F1 Score=0.8996\n",
            "Epoch  28/500 | Train: Loss=0.1227, F1 Score=0.9572 | Val: Loss=0.4287, F1 Score=0.8863\n",
            "Epoch  29/500 | Train: Loss=0.1136, F1 Score=0.9590 | Val: Loss=0.4497, F1 Score=0.8603\n",
            "Epoch  30/500 | Train: Loss=0.1250, F1 Score=0.9561 | Val: Loss=0.2891, F1 Score=0.9003\n",
            "Epoch  31/500 | Train: Loss=0.1065, F1 Score=0.9594 | Val: Loss=0.5892, F1 Score=0.8520\n",
            "Epoch  32/500 | Train: Loss=0.1475, F1 Score=0.9503 | Val: Loss=0.4619, F1 Score=0.8191\n",
            "Epoch  33/500 | Train: Loss=0.1928, F1 Score=0.9263 | Val: Loss=0.5284, F1 Score=0.8614\n",
            "Epoch  34/500 | Train: Loss=0.1123, F1 Score=0.9594 | Val: Loss=0.3117, F1 Score=0.9004\n",
            "Epoch  35/500 | Train: Loss=0.1966, F1 Score=0.9389 | Val: Loss=0.3666, F1 Score=0.9020\n",
            "Epoch  36/500 | Train: Loss=0.1298, F1 Score=0.9554 | Val: Loss=0.3790, F1 Score=0.8842\n",
            "Epoch  37/500 | Train: Loss=0.1105, F1 Score=0.9585 | Val: Loss=0.3214, F1 Score=0.9068\n",
            "Epoch  38/500 | Train: Loss=0.0757, F1 Score=0.9728 | Val: Loss=0.3356, F1 Score=0.9142\n",
            "Epoch  39/500 | Train: Loss=0.0866, F1 Score=0.9698 | Val: Loss=0.4399, F1 Score=0.8916\n",
            "Epoch  40/500 | Train: Loss=0.0740, F1 Score=0.9733 | Val: Loss=0.3257, F1 Score=0.9067\n",
            "Epoch  41/500 | Train: Loss=0.0763, F1 Score=0.9711 | Val: Loss=0.4665, F1 Score=0.8846\n",
            "Epoch  42/500 | Train: Loss=0.2579, F1 Score=0.9192 | Val: Loss=0.4676, F1 Score=0.8335\n",
            "Epoch  43/500 | Train: Loss=0.1651, F1 Score=0.9456 | Val: Loss=0.4555, F1 Score=0.8431\n",
            "Epoch  44/500 | Train: Loss=0.1243, F1 Score=0.9563 | Val: Loss=0.3779, F1 Score=0.8823\n",
            "Epoch  45/500 | Train: Loss=0.1246, F1 Score=0.9593 | Val: Loss=0.4783, F1 Score=0.8617\n",
            "Epoch  46/500 | Train: Loss=0.0993, F1 Score=0.9658 | Val: Loss=0.3697, F1 Score=0.8998\n",
            "Epoch  47/500 | Train: Loss=0.1278, F1 Score=0.9592 | Val: Loss=0.3986, F1 Score=0.8876\n",
            "Epoch  48/500 | Train: Loss=0.1000, F1 Score=0.9660 | Val: Loss=0.4458, F1 Score=0.8877\n",
            "Epoch  49/500 | Train: Loss=0.0770, F1 Score=0.9716 | Val: Loss=0.4227, F1 Score=0.8790\n",
            "Epoch  50/500 | Train: Loss=0.0882, F1 Score=0.9703 | Val: Loss=0.4059, F1 Score=0.8867\n",
            "Epoch  51/500 | Train: Loss=0.0696, F1 Score=0.9753 | Val: Loss=0.3481, F1 Score=0.9181\n",
            "Epoch  52/500 | Train: Loss=0.1035, F1 Score=0.9679 | Val: Loss=0.2768, F1 Score=0.9147\n",
            "Epoch  53/500 | Train: Loss=0.0789, F1 Score=0.9719 | Val: Loss=0.3764, F1 Score=0.9004\n",
            "Epoch  54/500 | Train: Loss=0.0596, F1 Score=0.9809 | Val: Loss=0.3214, F1 Score=0.9110\n",
            "Epoch  55/500 | Train: Loss=0.0566, F1 Score=0.9796 | Val: Loss=0.3862, F1 Score=0.8972\n",
            "Epoch  56/500 | Train: Loss=0.0551, F1 Score=0.9815 | Val: Loss=0.3374, F1 Score=0.9056\n",
            "Epoch  57/500 | Train: Loss=0.1422, F1 Score=0.9515 | Val: Loss=0.3789, F1 Score=0.8613\n",
            "Epoch  58/500 | Train: Loss=0.1295, F1 Score=0.9525 | Val: Loss=0.3685, F1 Score=0.8932\n",
            "Epoch  59/500 | Train: Loss=0.0939, F1 Score=0.9682 | Val: Loss=0.4188, F1 Score=0.8893\n",
            "Epoch  60/500 | Train: Loss=0.0876, F1 Score=0.9692 | Val: Loss=0.5198, F1 Score=0.8550\n",
            "Epoch  61/500 | Train: Loss=0.0581, F1 Score=0.9825 | Val: Loss=0.3323, F1 Score=0.9226\n",
            "Epoch  62/500 | Train: Loss=0.1057, F1 Score=0.9656 | Val: Loss=0.5534, F1 Score=0.8527\n",
            "Epoch  63/500 | Train: Loss=0.1098, F1 Score=0.9589 | Val: Loss=0.3877, F1 Score=0.8977\n",
            "Epoch  64/500 | Train: Loss=0.0647, F1 Score=0.9763 | Val: Loss=0.3821, F1 Score=0.8985\n",
            "Epoch  65/500 | Train: Loss=0.0738, F1 Score=0.9743 | Val: Loss=0.3224, F1 Score=0.9125\n",
            "Epoch  66/500 | Train: Loss=0.0819, F1 Score=0.9737 | Val: Loss=0.3841, F1 Score=0.8948\n",
            "Epoch  67/500 | Train: Loss=0.0641, F1 Score=0.9767 | Val: Loss=0.2668, F1 Score=0.9180\n",
            "Epoch  68/500 | Train: Loss=0.1641, F1 Score=0.9471 | Val: Loss=0.4821, F1 Score=0.8572\n",
            "Epoch  69/500 | Train: Loss=0.1186, F1 Score=0.9538 | Val: Loss=0.4135, F1 Score=0.8949\n",
            "Epoch  70/500 | Train: Loss=0.0662, F1 Score=0.9786 | Val: Loss=0.3936, F1 Score=0.8806\n",
            "Epoch  71/500 | Train: Loss=0.0446, F1 Score=0.9847 | Val: Loss=0.4028, F1 Score=0.9019\n",
            "Epoch  72/500 | Train: Loss=0.0468, F1 Score=0.9837 | Val: Loss=0.4138, F1 Score=0.9018\n",
            "Epoch  73/500 | Train: Loss=0.0505, F1 Score=0.9814 | Val: Loss=0.3775, F1 Score=0.8882\n",
            "Epoch  74/500 | Train: Loss=0.0546, F1 Score=0.9820 | Val: Loss=0.3455, F1 Score=0.9016\n",
            "Epoch  75/500 | Train: Loss=0.0629, F1 Score=0.9795 | Val: Loss=0.3679, F1 Score=0.9018\n",
            "Epoch  76/500 | Train: Loss=0.0526, F1 Score=0.9808 | Val: Loss=0.4104, F1 Score=0.9036\n",
            "Epoch  77/500 | Train: Loss=0.0791, F1 Score=0.9728 | Val: Loss=0.3531, F1 Score=0.9018\n",
            "Epoch  78/500 | Train: Loss=0.0398, F1 Score=0.9865 | Val: Loss=0.3684, F1 Score=0.9103\n",
            "Epoch  79/500 | Train: Loss=0.0478, F1 Score=0.9841 | Val: Loss=0.4547, F1 Score=0.8931\n",
            "Epoch  80/500 | Train: Loss=0.0415, F1 Score=0.9862 | Val: Loss=0.6470, F1 Score=0.8731\n",
            "Epoch  81/500 | Train: Loss=0.0827, F1 Score=0.9740 | Val: Loss=0.4911, F1 Score=0.8687\n",
            "Epoch  82/500 | Train: Loss=0.0561, F1 Score=0.9796 | Val: Loss=0.5720, F1 Score=0.8730\n",
            "Epoch  83/500 | Train: Loss=0.0629, F1 Score=0.9787 | Val: Loss=0.4591, F1 Score=0.8768\n",
            "Epoch  84/500 | Train: Loss=0.0408, F1 Score=0.9863 | Val: Loss=0.3536, F1 Score=0.9161\n",
            "Epoch  85/500 | Train: Loss=0.0619, F1 Score=0.9784 | Val: Loss=0.4336, F1 Score=0.8924\n",
            "Epoch  86/500 | Train: Loss=0.0372, F1 Score=0.9872 | Val: Loss=0.4368, F1 Score=0.8938\n",
            "Epoch  87/500 | Train: Loss=0.0425, F1 Score=0.9848 | Val: Loss=0.4914, F1 Score=0.8985\n",
            "Epoch  88/500 | Train: Loss=0.0301, F1 Score=0.9883 | Val: Loss=0.4498, F1 Score=0.9015\n",
            "Epoch  89/500 | Train: Loss=0.0908, F1 Score=0.9709 | Val: Loss=0.3360, F1 Score=0.9125\n",
            "Epoch  90/500 | Train: Loss=0.0448, F1 Score=0.9835 | Val: Loss=0.4300, F1 Score=0.9049\n",
            "Epoch  91/500 | Train: Loss=0.0451, F1 Score=0.9858 | Val: Loss=0.5454, F1 Score=0.8843\n",
            "Epoch  92/500 | Train: Loss=0.0507, F1 Score=0.9837 | Val: Loss=0.3948, F1 Score=0.9116\n",
            "Epoch  93/500 | Train: Loss=0.0304, F1 Score=0.9896 | Val: Loss=0.4026, F1 Score=0.9220\n",
            "Epoch  94/500 | Train: Loss=0.0616, F1 Score=0.9801 | Val: Loss=0.7888, F1 Score=0.8351\n",
            "Epoch  95/500 | Train: Loss=0.1325, F1 Score=0.9588 | Val: Loss=0.4429, F1 Score=0.9010\n",
            "Epoch  96/500 | Train: Loss=0.0353, F1 Score=0.9868 | Val: Loss=0.4318, F1 Score=0.9063\n",
            "Epoch  97/500 | Train: Loss=0.0424, F1 Score=0.9848 | Val: Loss=0.3937, F1 Score=0.9097\n",
            "Epoch  98/500 | Train: Loss=0.0457, F1 Score=0.9836 | Val: Loss=0.3758, F1 Score=0.8953\n",
            "Epoch  99/500 | Train: Loss=0.0489, F1 Score=0.9818 | Val: Loss=0.5375, F1 Score=0.8890\n",
            "Epoch 100/500 | Train: Loss=0.0406, F1 Score=0.9872 | Val: Loss=0.4439, F1 Score=0.8911\n",
            "Epoch 101/500 | Train: Loss=0.0257, F1 Score=0.9900 | Val: Loss=0.5382, F1 Score=0.8885\n",
            "Epoch 102/500 | Train: Loss=0.0312, F1 Score=0.9891 | Val: Loss=0.6871, F1 Score=0.8550\n",
            "Epoch 103/500 | Train: Loss=0.0686, F1 Score=0.9771 | Val: Loss=0.4967, F1 Score=0.8818\n",
            "Epoch 104/500 | Train: Loss=0.0274, F1 Score=0.9894 | Val: Loss=0.4309, F1 Score=0.9230\n",
            "Epoch 105/500 | Train: Loss=0.0230, F1 Score=0.9906 | Val: Loss=0.4812, F1 Score=0.8927\n",
            "Epoch 106/500 | Train: Loss=0.0293, F1 Score=0.9918 | Val: Loss=0.4722, F1 Score=0.9045\n",
            "Epoch 107/500 | Train: Loss=0.0256, F1 Score=0.9908 | Val: Loss=0.4647, F1 Score=0.8947\n",
            "Epoch 108/500 | Train: Loss=0.0329, F1 Score=0.9885 | Val: Loss=0.4489, F1 Score=0.9140\n",
            "Epoch 109/500 | Train: Loss=0.0483, F1 Score=0.9832 | Val: Loss=0.4312, F1 Score=0.9129\n",
            "Epoch 110/500 | Train: Loss=0.0189, F1 Score=0.9925 | Val: Loss=0.6056, F1 Score=0.9020\n",
            "Epoch 111/500 | Train: Loss=0.0266, F1 Score=0.9896 | Val: Loss=0.6301, F1 Score=0.8780\n",
            "Epoch 112/500 | Train: Loss=0.0428, F1 Score=0.9869 | Val: Loss=0.5850, F1 Score=0.8762\n",
            "Epoch 113/500 | Train: Loss=0.0657, F1 Score=0.9797 | Val: Loss=0.5093, F1 Score=0.8861\n",
            "Epoch 114/500 | Train: Loss=0.0624, F1 Score=0.9804 | Val: Loss=0.4535, F1 Score=0.9012\n",
            "Epoch 115/500 | Train: Loss=0.0302, F1 Score=0.9886 | Val: Loss=0.5517, F1 Score=0.8769\n",
            "Epoch 116/500 | Train: Loss=0.0263, F1 Score=0.9908 | Val: Loss=0.5196, F1 Score=0.8778\n",
            "Epoch 117/500 | Train: Loss=0.0378, F1 Score=0.9873 | Val: Loss=0.5408, F1 Score=0.8860\n",
            "Epoch 118/500 | Train: Loss=0.0284, F1 Score=0.9899 | Val: Loss=0.6049, F1 Score=0.8741\n",
            "Epoch 119/500 | Train: Loss=0.0561, F1 Score=0.9813 | Val: Loss=0.4549, F1 Score=0.9046\n",
            "Epoch 120/500 | Train: Loss=0.0238, F1 Score=0.9916 | Val: Loss=0.5136, F1 Score=0.9058\n",
            "Epoch 121/500 | Train: Loss=0.0402, F1 Score=0.9863 | Val: Loss=0.4594, F1 Score=0.9067\n",
            "Epoch 122/500 | Train: Loss=0.0401, F1 Score=0.9855 | Val: Loss=0.5308, F1 Score=0.9019\n",
            "Epoch 123/500 | Train: Loss=0.0228, F1 Score=0.9917 | Val: Loss=0.5165, F1 Score=0.9154\n",
            "Epoch 124/500 | Train: Loss=0.0925, F1 Score=0.9734 | Val: Loss=0.4220, F1 Score=0.9134\n",
            "Epoch 125/500 | Train: Loss=0.0351, F1 Score=0.9872 | Val: Loss=0.4875, F1 Score=0.8906\n",
            "Epoch 126/500 | Train: Loss=0.0281, F1 Score=0.9902 | Val: Loss=0.4897, F1 Score=0.8985\n",
            "Epoch 127/500 | Train: Loss=0.0468, F1 Score=0.9842 | Val: Loss=0.6888, F1 Score=0.8558\n",
            "Epoch 128/500 | Train: Loss=0.0218, F1 Score=0.9922 | Val: Loss=0.5593, F1 Score=0.9038\n",
            "Epoch 129/500 | Train: Loss=0.0425, F1 Score=0.9859 | Val: Loss=0.5305, F1 Score=0.8933\n",
            "Epoch 130/500 | Train: Loss=0.0507, F1 Score=0.9845 | Val: Loss=0.4869, F1 Score=0.8841\n",
            "Epoch 131/500 | Train: Loss=0.0503, F1 Score=0.9832 | Val: Loss=0.5635, F1 Score=0.8590\n",
            "Epoch 132/500 | Train: Loss=0.0368, F1 Score=0.9878 | Val: Loss=0.4704, F1 Score=0.9141\n",
            "Epoch 133/500 | Train: Loss=0.0210, F1 Score=0.9923 | Val: Loss=0.4678, F1 Score=0.9208\n",
            "Epoch 134/500 | Train: Loss=0.0188, F1 Score=0.9920 | Val: Loss=0.4405, F1 Score=0.9221\n",
            "Epoch 135/500 | Train: Loss=0.0194, F1 Score=0.9928 | Val: Loss=0.4235, F1 Score=0.9238\n",
            "Epoch 136/500 | Train: Loss=0.0433, F1 Score=0.9853 | Val: Loss=0.3950, F1 Score=0.9170\n",
            "Epoch 137/500 | Train: Loss=0.0236, F1 Score=0.9911 | Val: Loss=0.5972, F1 Score=0.8822\n",
            "Epoch 138/500 | Train: Loss=0.0311, F1 Score=0.9894 | Val: Loss=0.4577, F1 Score=0.8980\n",
            "Epoch 139/500 | Train: Loss=0.0636, F1 Score=0.9791 | Val: Loss=0.5224, F1 Score=0.8485\n",
            "Epoch 140/500 | Train: Loss=0.0620, F1 Score=0.9781 | Val: Loss=0.4250, F1 Score=0.8982\n",
            "Epoch 141/500 | Train: Loss=0.0769, F1 Score=0.9801 | Val: Loss=0.6172, F1 Score=0.8323\n",
            "Epoch 142/500 | Train: Loss=0.1087, F1 Score=0.9674 | Val: Loss=0.4258, F1 Score=0.8895\n",
            "Epoch 143/500 | Train: Loss=0.0490, F1 Score=0.9829 | Val: Loss=0.4660, F1 Score=0.8826\n",
            "Epoch 144/500 | Train: Loss=0.0347, F1 Score=0.9867 | Val: Loss=0.6258, F1 Score=0.8854\n",
            "Epoch 145/500 | Train: Loss=0.0210, F1 Score=0.9925 | Val: Loss=0.4898, F1 Score=0.9115\n",
            "Epoch 146/500 | Train: Loss=0.0428, F1 Score=0.9854 | Val: Loss=0.5126, F1 Score=0.8945\n",
            "Epoch 147/500 | Train: Loss=0.0249, F1 Score=0.9913 | Val: Loss=0.5369, F1 Score=0.9065\n",
            "Epoch 148/500 | Train: Loss=0.0309, F1 Score=0.9889 | Val: Loss=0.5342, F1 Score=0.9021\n",
            "Epoch 149/500 | Train: Loss=0.0301, F1 Score=0.9900 | Val: Loss=0.4191, F1 Score=0.9184\n",
            "Epoch 150/500 | Train: Loss=0.0159, F1 Score=0.9941 | Val: Loss=0.5336, F1 Score=0.8967\n",
            "Epoch 151/500 | Train: Loss=0.0255, F1 Score=0.9906 | Val: Loss=0.7499, F1 Score=0.8633\n",
            "Epoch 152/500 | Train: Loss=0.0575, F1 Score=0.9795 | Val: Loss=0.5108, F1 Score=0.8882\n",
            "Epoch 153/500 | Train: Loss=0.0217, F1 Score=0.9936 | Val: Loss=0.8456, F1 Score=0.8525\n",
            "Epoch 154/500 | Train: Loss=0.0249, F1 Score=0.9912 | Val: Loss=0.6053, F1 Score=0.8829\n",
            "Epoch 155/500 | Train: Loss=0.0362, F1 Score=0.9871 | Val: Loss=0.5343, F1 Score=0.9019\n",
            "Epoch 156/500 | Train: Loss=0.0525, F1 Score=0.9837 | Val: Loss=0.7804, F1 Score=0.8533\n",
            "Epoch 157/500 | Train: Loss=0.0276, F1 Score=0.9899 | Val: Loss=0.6360, F1 Score=0.8814\n",
            "Epoch 158/500 | Train: Loss=0.0184, F1 Score=0.9935 | Val: Loss=0.6691, F1 Score=0.8835\n",
            "Epoch 159/500 | Train: Loss=0.0196, F1 Score=0.9922 | Val: Loss=0.6787, F1 Score=0.8869\n",
            "Epoch 160/500 | Train: Loss=0.0180, F1 Score=0.9938 | Val: Loss=0.6622, F1 Score=0.8830\n",
            "Epoch 161/500 | Train: Loss=0.0241, F1 Score=0.9916 | Val: Loss=0.6601, F1 Score=0.8792\n",
            "Epoch 162/500 | Train: Loss=0.0592, F1 Score=0.9818 | Val: Loss=0.5398, F1 Score=0.8923\n",
            "Epoch 163/500 | Train: Loss=0.0195, F1 Score=0.9927 | Val: Loss=0.5530, F1 Score=0.8902\n",
            "Epoch 164/500 | Train: Loss=0.0147, F1 Score=0.9938 | Val: Loss=0.5934, F1 Score=0.9043\n",
            "Epoch 165/500 | Train: Loss=0.0434, F1 Score=0.9852 | Val: Loss=0.6115, F1 Score=0.8741\n",
            "Epoch 166/500 | Train: Loss=0.0275, F1 Score=0.9909 | Val: Loss=0.5516, F1 Score=0.8944\n",
            "Epoch 167/500 | Train: Loss=0.0164, F1 Score=0.9930 | Val: Loss=0.6593, F1 Score=0.8686\n",
            "Epoch 168/500 | Train: Loss=0.0164, F1 Score=0.9935 | Val: Loss=0.6066, F1 Score=0.8945\n",
            "Epoch 169/500 | Train: Loss=0.0133, F1 Score=0.9954 | Val: Loss=0.7119, F1 Score=0.8684\n",
            "Epoch 170/500 | Train: Loss=0.0183, F1 Score=0.9925 | Val: Loss=0.6346, F1 Score=0.8736\n",
            "Epoch 171/500 | Train: Loss=0.0129, F1 Score=0.9949 | Val: Loss=0.6205, F1 Score=0.9050\n",
            "Epoch 172/500 | Train: Loss=0.0099, F1 Score=0.9965 | Val: Loss=0.5900, F1 Score=0.9171\n",
            "Epoch 173/500 | Train: Loss=0.0164, F1 Score=0.9948 | Val: Loss=0.6227, F1 Score=0.8980\n",
            "Epoch 174/500 | Train: Loss=0.0744, F1 Score=0.9788 | Val: Loss=0.6867, F1 Score=0.8700\n",
            "Epoch 175/500 | Train: Loss=0.0261, F1 Score=0.9905 | Val: Loss=0.5506, F1 Score=0.9000\n",
            "Epoch 176/500 | Train: Loss=0.0157, F1 Score=0.9948 | Val: Loss=0.5795, F1 Score=0.8921\n",
            "Epoch 177/500 | Train: Loss=0.0153, F1 Score=0.9951 | Val: Loss=0.6720, F1 Score=0.8866\n",
            "Epoch 178/500 | Train: Loss=0.0739, F1 Score=0.9831 | Val: Loss=0.4120, F1 Score=0.8908\n",
            "Epoch 179/500 | Train: Loss=0.0303, F1 Score=0.9896 | Val: Loss=0.5520, F1 Score=0.8994\n",
            "Epoch 180/500 | Train: Loss=0.0147, F1 Score=0.9956 | Val: Loss=0.4547, F1 Score=0.9248\n",
            "Epoch 181/500 | Train: Loss=0.0270, F1 Score=0.9906 | Val: Loss=0.5059, F1 Score=0.8844\n",
            "Epoch 182/500 | Train: Loss=0.0310, F1 Score=0.9897 | Val: Loss=0.6525, F1 Score=0.8685\n",
            "Epoch 183/500 | Train: Loss=0.0234, F1 Score=0.9926 | Val: Loss=0.5648, F1 Score=0.9092\n",
            "Epoch 184/500 | Train: Loss=0.0109, F1 Score=0.9961 | Val: Loss=0.5613, F1 Score=0.9155\n",
            "Epoch 185/500 | Train: Loss=0.0162, F1 Score=0.9950 | Val: Loss=0.5215, F1 Score=0.9161\n",
            "Epoch 186/500 | Train: Loss=0.0094, F1 Score=0.9964 | Val: Loss=0.6700, F1 Score=0.8945\n",
            "Epoch 187/500 | Train: Loss=0.0856, F1 Score=0.9784 | Val: Loss=0.6271, F1 Score=0.8026\n",
            "Epoch 188/500 | Train: Loss=0.1137, F1 Score=0.9608 | Val: Loss=0.5135, F1 Score=0.8625\n",
            "Epoch 189/500 | Train: Loss=0.0314, F1 Score=0.9892 | Val: Loss=0.6370, F1 Score=0.8619\n",
            "Epoch 190/500 | Train: Loss=0.0187, F1 Score=0.9937 | Val: Loss=0.6767, F1 Score=0.8678\n",
            "Epoch 191/500 | Train: Loss=0.0182, F1 Score=0.9939 | Val: Loss=0.5808, F1 Score=0.8904\n",
            "Epoch 192/500 | Train: Loss=0.0223, F1 Score=0.9919 | Val: Loss=0.6267, F1 Score=0.8762\n",
            "Epoch 193/500 | Train: Loss=0.0163, F1 Score=0.9943 | Val: Loss=0.6752, F1 Score=0.8794\n",
            "Epoch 194/500 | Train: Loss=0.0136, F1 Score=0.9955 | Val: Loss=0.6694, F1 Score=0.8918\n",
            "Epoch 195/500 | Train: Loss=0.0230, F1 Score=0.9928 | Val: Loss=0.5306, F1 Score=0.8869\n",
            "Epoch 196/500 | Train: Loss=0.0123, F1 Score=0.9955 | Val: Loss=0.5756, F1 Score=0.8994\n",
            "Epoch 197/500 | Train: Loss=0.0144, F1 Score=0.9942 | Val: Loss=0.6489, F1 Score=0.8888\n",
            "Epoch 198/500 | Train: Loss=0.1438, F1 Score=0.9586 | Val: Loss=0.4078, F1 Score=0.8663\n",
            "Epoch 199/500 | Train: Loss=0.0555, F1 Score=0.9816 | Val: Loss=0.8405, F1 Score=0.8170\n",
            "Epoch 200/500 | Train: Loss=0.0300, F1 Score=0.9897 | Val: Loss=0.6154, F1 Score=0.8841\n",
            "Epoch 201/500 | Train: Loss=0.0128, F1 Score=0.9953 | Val: Loss=0.6759, F1 Score=0.8825\n",
            "Epoch 202/500 | Train: Loss=0.0242, F1 Score=0.9909 | Val: Loss=0.5814, F1 Score=0.9111\n",
            "Epoch 203/500 | Train: Loss=0.0128, F1 Score=0.9948 | Val: Loss=0.6056, F1 Score=0.9056\n",
            "Epoch 204/500 | Train: Loss=0.0141, F1 Score=0.9950 | Val: Loss=0.8247, F1 Score=0.8772\n",
            "Epoch 205/500 | Train: Loss=0.0147, F1 Score=0.9950 | Val: Loss=0.8507, F1 Score=0.8550\n",
            "Epoch 206/500 | Train: Loss=0.0152, F1 Score=0.9948 | Val: Loss=0.6869, F1 Score=0.8895\n",
            "Epoch 207/500 | Train: Loss=0.0103, F1 Score=0.9963 | Val: Loss=0.7457, F1 Score=0.8875\n",
            "Epoch 208/500 | Train: Loss=0.0114, F1 Score=0.9960 | Val: Loss=0.6378, F1 Score=0.9045\n",
            "Epoch 209/500 | Train: Loss=0.0346, F1 Score=0.9893 | Val: Loss=0.6548, F1 Score=0.8815\n",
            "Epoch 210/500 | Train: Loss=0.0258, F1 Score=0.9916 | Val: Loss=0.5990, F1 Score=0.9175\n",
            "Epoch 211/500 | Train: Loss=0.0514, F1 Score=0.9835 | Val: Loss=0.6275, F1 Score=0.8442\n",
            "Epoch 212/500 | Train: Loss=0.0246, F1 Score=0.9911 | Val: Loss=0.6004, F1 Score=0.8919\n",
            "Epoch 213/500 | Train: Loss=0.0126, F1 Score=0.9956 | Val: Loss=0.7315, F1 Score=0.8785\n",
            "Epoch 214/500 | Train: Loss=0.0162, F1 Score=0.9938 | Val: Loss=0.6131, F1 Score=0.9131\n",
            "Epoch 215/500 | Train: Loss=0.0118, F1 Score=0.9956 | Val: Loss=0.6461, F1 Score=0.9082\n",
            "Epoch 216/500 | Train: Loss=0.0080, F1 Score=0.9973 | Val: Loss=0.7082, F1 Score=0.9061\n",
            "Epoch 217/500 | Train: Loss=0.0073, F1 Score=0.9975 | Val: Loss=0.6750, F1 Score=0.9083\n",
            "Epoch 218/500 | Train: Loss=0.0460, F1 Score=0.9874 | Val: Loss=0.6308, F1 Score=0.8800\n",
            "Epoch 219/500 | Train: Loss=0.0249, F1 Score=0.9912 | Val: Loss=0.7856, F1 Score=0.8609\n",
            "Epoch 220/500 | Train: Loss=0.0262, F1 Score=0.9907 | Val: Loss=0.6802, F1 Score=0.8944\n",
            "Epoch 221/500 | Train: Loss=0.0077, F1 Score=0.9976 | Val: Loss=0.8183, F1 Score=0.8769\n",
            "Epoch 222/500 | Train: Loss=0.0069, F1 Score=0.9976 | Val: Loss=0.7372, F1 Score=0.9015\n",
            "Epoch 223/500 | Train: Loss=0.0114, F1 Score=0.9953 | Val: Loss=0.7967, F1 Score=0.8944\n",
            "Epoch 224/500 | Train: Loss=0.0585, F1 Score=0.9849 | Val: Loss=0.8568, F1 Score=0.8420\n",
            "Epoch 225/500 | Train: Loss=0.0378, F1 Score=0.9877 | Val: Loss=0.6618, F1 Score=0.9013\n",
            "Epoch 226/500 | Train: Loss=0.0092, F1 Score=0.9973 | Val: Loss=0.7806, F1 Score=0.8900\n",
            "Epoch 227/500 | Train: Loss=0.0071, F1 Score=0.9973 | Val: Loss=0.7627, F1 Score=0.8946\n",
            "Epoch 228/500 | Train: Loss=0.0077, F1 Score=0.9970 | Val: Loss=0.7592, F1 Score=0.9006\n",
            "Epoch 229/500 | Train: Loss=0.0115, F1 Score=0.9955 | Val: Loss=1.0051, F1 Score=0.8800\n",
            "Epoch 230/500 | Train: Loss=0.0203, F1 Score=0.9933 | Val: Loss=0.6386, F1 Score=0.8935\n",
            "Early stopping triggered after 230 epochs.\n",
            "Best model restored from epoch 180 with val_f1 0.9248\n",
            "CPU times: user 7min 54s, sys: 36.2 s, total: 8min 30s\n",
            "Wall time: 8min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Train model and track training history\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=1,\n",
        "    experiment_name=\"bi_lstm\",\n",
        "    patience=PATIENCE\n",
        "    )\n",
        "\n",
        "# Update best model if current performance is superior\n",
        "if training_history['val_f1'][-1] > best_performance:\n",
        "    best_model = rnn_model\n",
        "    best_performance = training_history['val_f1'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YxWzzUO0fCY6"
      },
      "outputs": [],
      "source": [
        "# @title Plot Hitory\n",
        "# Create a figure with two side-by-side subplots (two columns)\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "# Plot of training and validation loss on the first axis\n",
        "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot of training and validation accuracy on the second axis\n",
        "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
        "ax2.set_title('F1 Score')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oM09gbAJfCWK"
      },
      "outputs": [],
      "source": [
        "# @title Plot Confusion Matrix\n",
        "# Collect predictions and ground truth labels\n",
        "val_preds, val_targets = [], []\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        logits = rnn_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store batch results\n",
        "        val_preds.append(preds)\n",
        "        val_targets.append(yb.numpy())\n",
        "\n",
        "# Combine all batches into single arrays\n",
        "val_preds = np.concatenate(val_preds)\n",
        "val_targets = np.concatenate(val_targets)\n",
        "\n",
        "# Calculate overall validation metrics\n",
        "val_acc = accuracy_score(val_targets, val_preds)\n",
        "val_prec = precision_score(val_targets, val_preds, average='weighted')\n",
        "val_rec = recall_score(val_targets, val_preds, average='weighted')\n",
        "val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
        "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
        "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
        "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
        "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for detailed error analysis\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "# Create numeric labels for heatmap annotation\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm, annot=labels, fmt='',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix ‚Äî Validation Set')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0ASP2DRnOiJT"
      },
      "outputs": [],
      "source": [
        "# @title Plot Hitory\n",
        "# Create a figure with two side-by-side subplots (two columns)\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "# Plot of training and validation loss on the first axis\n",
        "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot of training and validation accuracy on the second axis\n",
        "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
        "ax2.set_title('F1 Score')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hufUEx5ik-P7"
      },
      "outputs": [],
      "source": [
        "# Copy TensorBoard logs to accessible location for Colab\n",
        "!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n",
        "\n",
        "# Launch TensorBoard interface\n",
        "%tensorboard --logdir \"/content/\"$logs_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5VrQa_OMu74"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gx84yscPhkp"
      },
      "source": [
        "## **K-Shuffle-Split Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8VYpD2qHXYf"
      },
      "outputs": [],
      "source": [
        "# Cross-validation\n",
        "K = 5                    # Number of splits (5 and 10 are considered good values)\n",
        "N_VAL_USERS = 5          # Number of users for validation split\n",
        "N_TEST_USERS = 5         # Number of users for test split\n",
        "\n",
        "# Training\n",
        "EPOCHS = 500             # Maximum epochs (increase to improve performance)\n",
        "PATIENCE = 50            # Early stopping patience (increase to improve performance)\n",
        "VERBOSE = 10             # Print frequency\n",
        "\n",
        "# Optimisation\n",
        "LEARNING_RATE = 1e-3     # Learning rate\n",
        "BATCH_SIZE = 512         # Batch size\n",
        "WINDOW_SIZE = 200        # Input window size\n",
        "STRIDE = 50              # Input stride\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = 128        # Neurons per layer\n",
        "RNN_TYPE = 'GRU'         # Type of RNN architecture\n",
        "BIDIRECTIONAL = False    # Bidirectional RNN\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.2       # Dropout probability\n",
        "L1_LAMBDA = 0            # L1 penalty\n",
        "L2_LAMBDA = 0            # L2 penalty\n",
        "\n",
        "# Training utilities\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFfH7LKpHXWH"
      },
      "outputs": [],
      "source": [
        "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
        "                            k, n_val_users, n_test_users, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
        "                            window_size, stride, rnn_type, bidirectional,\n",
        "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        epochs: Number of training epochs\n",
        "        criterion: Loss function\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        n_val_users: Number of users for validation set\n",
        "        n_test_users: Number of users for test set\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        dropout_rate: Dropout rate\n",
        "        window_size: Length of sliding windows\n",
        "        stride: Step size for sliding windows\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verbosity level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "\n",
        "    # Get model architecture parameters\n",
        "    in_features = 3  # x_axis, y_axis, z_axis\n",
        "    num_classes = len(df['activity'].unique())\n",
        "\n",
        "    # Initialise model architecture\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=in_features,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=hidden_layers,\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=dropout_rate,\n",
        "        bidirectional=bidirectional,\n",
        "        rnn_type=rnn_type\n",
        "    ).to(device)\n",
        "\n",
        "    # Store initial weights to reset model for each split\n",
        "    initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Iterate through K random splits\n",
        "    for split_idx in range(k):\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Split {split_idx+1}/{k}\")\n",
        "\n",
        "        # Get unique user IDs and shuffle them with split-specific seed\n",
        "        unique_users = df['user_id'].unique()\n",
        "        random.seed(seed + split_idx)\n",
        "        random.shuffle(unique_users)\n",
        "\n",
        "        # Calculate the number of users for the training set\n",
        "        n_train_users = len(unique_users) - n_val_users - n_test_users\n",
        "\n",
        "        # Split the shuffled user IDs into training, validation, and test sets\n",
        "        train_users = unique_users[:n_train_users]\n",
        "        val_users = unique_users[n_train_users:n_train_users + n_val_users]\n",
        "        test_users = unique_users[n_train_users + n_val_users:]\n",
        "\n",
        "        # Split the dataset into training, validation, and test sets based on user IDs\n",
        "        df_train = df[df['user_id'].isin(train_users)].copy()\n",
        "        df_val = df[df['user_id'].isin(val_users)].copy()\n",
        "        df_test = df[df['user_id'].isin(test_users)].copy()\n",
        "\n",
        "        # Define a mapping of activity names to integer labels\n",
        "        label_mapping = {\n",
        "            'Walking': 0,\n",
        "            'Jogging': 1,\n",
        "            'Upstairs': 2,\n",
        "            'Downstairs': 3,\n",
        "            'Sitting': 4,\n",
        "            'Standing': 5\n",
        "        }\n",
        "\n",
        "        # Map activity names to integers in the training set\n",
        "        df_train['activity'] = df_train['activity'].map(label_mapping)\n",
        "\n",
        "        # Map activity names to integers in the validation set\n",
        "        df_val['activity'] = df_val['activity'].map(label_mapping)\n",
        "\n",
        "        # Map activity names to integers in the test set\n",
        "        df_test['activity'] = df_test['activity'].map(label_mapping)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training set shape: {df_train.shape}\")\n",
        "            print(f\"  Validation set shape: {df_val.shape}\")\n",
        "            print(f\"  Test set shape: {df_test.shape}\")\n",
        "\n",
        "        # Normalise features using training set statistics\n",
        "        train_max = df_train[['x_axis', 'y_axis', 'z_axis']].max()\n",
        "        train_min = df_train[['x_axis', 'y_axis', 'z_axis']].min()\n",
        "\n",
        "        df_train[['x_axis', 'y_axis', 'z_axis']] = (df_train[['x_axis', 'y_axis', 'z_axis']] - train_min) / (train_max - train_min + 1e-8)\n",
        "        df_val[['x_axis', 'y_axis', 'z_axis']] = (df_val[['x_axis', 'y_axis', 'z_axis']] - train_min) / (train_max - train_min + 1e-8)\n",
        "        df_test[['x_axis', 'y_axis', 'z_axis']] = (df_test[['x_axis', 'y_axis', 'z_axis']] - train_min) / (train_max - train_min + 1e-8)\n",
        "\n",
        "        # Build sequences using the existing build_sequences function\n",
        "        X_train, y_train = build_sequences(df_train, window=window_size, stride=stride)\n",
        "        X_val, y_val = build_sequences(df_val, window=window_size, stride=stride)\n",
        "        X_test, y_test = build_sequences(df_test, window=window_size, stride=stride)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training sequences shape: {X_train.shape}\")\n",
        "            print(f\"  Validation sequences shape: {X_val.shape}\")\n",
        "            print(f\"  Test sequences shape: {X_test.shape}\")\n",
        "\n",
        "        # Create PyTorch datasets\n",
        "        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "        val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "        test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "        test_loader  = make_loader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "        # Reset model to initial weights for fair comparison across splits\n",
        "        model.load_state_dict(initial_state)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        # Enable mixed precision training for GPU acceleration\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Train model on current split\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            l1_lambda=l1_lambda,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
        "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}¬±{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw59lTVZHXTs"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Execute K-fold cross-validation with baseline configuration\n",
        "losses, metrics, best_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "    df=df,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    k=K,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    hidden_layers=HIDDEN_LAYERS,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    l1_lambda=L1_LAMBDA,\n",
        "    l2_lambda=L2_LAMBDA,\n",
        "    verbose=VERBOSE,\n",
        "    patience=PATIENCE,\n",
        "    seed=SEED,\n",
        "    experiment_name=\"gru_baseline\",\n",
        "    n_val_users=N_VAL_USERS,\n",
        "    n_test_users=N_TEST_USERS,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    stride=STRIDE,\n",
        "    rnn_type=RNN_TYPE,\n",
        "    bidirectional=BIDIRECTIONAL\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fq7WbKm0NuDv"
      },
      "outputs": [],
      "source": [
        "# @title Plot Hitory\n",
        "# Create figure with two subplots sharing x axis\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5), sharex=True)\n",
        "\n",
        "# Color palette for K splits\n",
        "colors = plt.cm.get_cmap('tab10', K)\n",
        "\n",
        "# Plot validation loss for each split\n",
        "for split in range(K):\n",
        "    axes[0].plot(losses[f'split_{split}'][:-PATIENCE], label=f'Split {split+1}',\n",
        "                 color=colors(split), alpha=0.6)\n",
        "axes[0].set_title('Validation Loss per Split')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot validation F1 score for each split\n",
        "for split in range(K):\n",
        "    axes[1].plot(metrics[f'split_{split}'][:-PATIENCE], label=f'Split {split+1}',\n",
        "                 color=colors(split), alpha=0.6)\n",
        "axes[1].set_title('Validation F1 Score per Split')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Add shared legend on the right\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.975)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpMYvRFvNuDv"
      },
      "source": [
        "## **Hyperparameters Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPlepEPwHXOo"
      },
      "outputs": [],
      "source": [
        "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True):\n",
        "    \"\"\"\n",
        "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
        "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
        "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
        "        verbose: Print progress for each configuration\n",
        "\n",
        "    Returns:\n",
        "        results: Dict with scores for each configuration\n",
        "        best_config: Dict with best hyperparameter combination\n",
        "        best_score: Best mean F1 score achieved\n",
        "    \"\"\"\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    total = len(combinations)\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "        # Create current configuration dict\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
        "            for param, value in current_config.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "\n",
        "        # Merge current config with fixed parameters\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Execute cross-validation\n",
        "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "        # Track best configuration\n",
        "        if fold_scores[\"mean\"] > best_score:\n",
        "            best_score = fold_scores[\"mean\"]\n",
        "            best_config = current_config.copy()\n",
        "            if verbose:\n",
        "                print(\"  NEW BEST SCORE!\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  F1 Score: {fold_scores['mean']:.4f}¬±{fold_scores['std']:.4f}\")\n",
        "\n",
        "    return results, best_config, best_score\n",
        "\n",
        "\n",
        "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of results from grid_search_cv_rnn\n",
        "        k_splits: Number of CV splits used\n",
        "        top_n: Number of top configurations to display\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    # Sort by mean score\n",
        "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
        "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top N\n",
        "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
        "\n",
        "    # Prepare boxplot data\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
        "    replacements = {\n",
        "        'batch_size_': 'BS=',\n",
        "        'learning_rate_': '\\nLR=',\n",
        "        'hidden_layers_': '\\nHL=',\n",
        "        'hidden_size_': '\\nHS=',\n",
        "        'dropout_rate_': '\\nDR=',\n",
        "        'window_size_': '\\nWS=',\n",
        "        'stride_': '\\nSTR=',\n",
        "        'rnn_type_': '\\nRNN=',\n",
        "        'bidirectional_': '\\nBIDIR=',\n",
        "        'l1_lambda_': '\\nL1=',\n",
        "        'l2_lambda_': '\\nL2='\n",
        "    }\n",
        "\n",
        "    # Replacements for separators\n",
        "    separator_replacements = {\n",
        "        '_learning_rate_': '\\nLR=',\n",
        "        '_hidden_layers_': '\\nHL=',\n",
        "        '_hidden_size_': '\\nHS=',\n",
        "        '_dropout_rate_': '\\nDR=',\n",
        "        '_window_size_': '\\nWS=',\n",
        "        '_stride_': '\\nSTR=',\n",
        "        '_rnn_type_': '\\nRNN=',\n",
        "        '_bidirectional_': '\\nBIDIR=',\n",
        "        '_l1_lambda_': '\\nL1=',\n",
        "        '_l2_lambda_': '\\nL2=',\n",
        "        '_': ''\n",
        "    }\n",
        "\n",
        "    for config_name, mean_score in top_configs:\n",
        "        # Extract best score from each split (auto-detect number of splits)\n",
        "        split_scores = []\n",
        "        for i in range(k_splits):\n",
        "            if f'split_{i}' in results[config_name]:\n",
        "                split_scores.append(results[config_name][f'split_{i}'])\n",
        "        boxplot_data.append(split_scores)\n",
        "\n",
        "        # Verify we have the expected number of splits\n",
        "        if len(split_scores) != k_splits:\n",
        "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
        "\n",
        "        # Create readable label using the replacements dictionary\n",
        "        readable_label = config_name\n",
        "        for old, new in replacements.items():\n",
        "            readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        # Apply separator replacements\n",
        "        for old, new in separator_replacements.items():\n",
        "             readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        labels.append(f\"{readable_label}\\n(Œº={mean_score:.3f})\")\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
        "                    showmeans=True, meanline=True)\n",
        "\n",
        "    # Styling\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # Highlight best configuration\n",
        "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
        "\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "    plt.xticks(rotation=0, ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbD8awRDHXML"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Define parameters to search\n",
        "param_grid = {\n",
        "    'window_size': [50, 100, 200, 400],\n",
        "    'stride': [25, 50],\n",
        "}\n",
        "\n",
        "# Fixed hyperparameters (not being tuned)\n",
        "fixed_params = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'hidden_layers': HIDDEN_LAYERS,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'dropout_rate': DROPOUT_RATE,\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "    'l2_lambda': L2_LAMBDA,\n",
        "    'rnn_type': RNN_TYPE,\n",
        "    'bidirectional': BIDIRECTIONAL\n",
        "}\n",
        "\n",
        "# Cross-validation settings\n",
        "cv_params = {\n",
        "    'epochs': EPOCHS,\n",
        "    'criterion': criterion,\n",
        "    'device': device,\n",
        "    'k': K,\n",
        "    'n_val_users': N_VAL_USERS,\n",
        "    'n_test_users': N_TEST_USERS,\n",
        "    'patience': PATIENCE,\n",
        "    'verbose': 0,\n",
        "    'seed': SEED\n",
        "}\n",
        "\n",
        "# Execute search\n",
        "results, best_config, best_score = grid_search_cv_rnn(\n",
        "    df=df,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    cv_params=cv_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImYxq6AsHXJv"
      },
      "outputs": [],
      "source": [
        "# Visualise results\n",
        "plot_top_configurations_rnn(results, k_splits=K, top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eytiPUW_NuDw"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# --- 1. Combine fixed and best hyperparameters ---\n",
        "# 'fixed_params' and 'best_config' are loaded from the grid search cell\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "\n",
        "# Generate config string (from grid params only) to find saved model files\n",
        "best_config_str = \"_\".join([f\"{k}_{v}\" for k, v in best_config.items()])\n",
        "\n",
        "# Initialise lists for metrics\n",
        "test_accuracies = []\n",
        "test_precisions = []\n",
        "test_recall_scores = []\n",
        "test_f1_scores = []\n",
        "all_test_targets = [] # For aggregated confusion matrix\n",
        "all_test_preds = []   # For aggregated confusion matrix\n",
        "\n",
        "label_mapping = {\n",
        "    'Walking': 0, 'Jogging': 1, 'Upstairs': 2,\n",
        "    'Downstairs': 3, 'Sitting': 4, 'Standing': 5\n",
        "}\n",
        "scale_columns = ['x_axis', 'y_axis', 'z_axis']\n",
        "\n",
        "# --- 2. Begin evaluation loop across the K splits ---\n",
        "# K, SEED, N_VAL_USERS, N_TEST_USERS are defined globally\n",
        "for split in range(K):\n",
        "    print(f\"Evaluating Split {split+1}/{K} using best config: {best_config_str}\")\n",
        "\n",
        "    # --- 3. Regenerate the exact data split for this fold ---\n",
        "    # This logic must be identical to k_shuffle_split_cross_validation_round_rnn\n",
        "    unique_users = df['user_id'].unique()\n",
        "    random.seed(SEED + split) # Use the same CV seed\n",
        "    random.shuffle(unique_users)\n",
        "\n",
        "    n_train_users = len(unique_users) - N_VAL_USERS - N_TEST_USERS\n",
        "    train_users = unique_users[:n_train_users]\n",
        "    val_users = unique_users[n_train_users:n_train_users + N_VAL_USERS]\n",
        "    test_users = unique_users[n_train_users + N_VAL_USERS:]\n",
        "\n",
        "    df_train = df[df['user_id'].isin(train_users)].copy()\n",
        "    df_test = df[df['user_id'].isin(test_users)].copy()\n",
        "\n",
        "    # --- 4. Preprocess the data ---\n",
        "    df_train['activity'] = df_train['activity'].map(label_mapping)\n",
        "    df_test['activity'] = df_test['activity'].map(label_mapping)\n",
        "\n",
        "    # Normalise features (fit on THIS split's training data)\n",
        "    mins = df_train[scale_columns].min()\n",
        "    maxs = df_train[scale_columns].max()\n",
        "\n",
        "    for column in scale_columns:\n",
        "        df_test[column] = (df_test[column] - mins[column]) / (maxs[column] - mins[column] + 1e-8)\n",
        "\n",
        "    # --- 5. Build test sequences ---\n",
        "    # Use the best window/stride from final_best_params\n",
        "    X_test, y_test = build_sequences(\n",
        "        df_test,\n",
        "        window=final_best_params['window_size'],\n",
        "        stride=final_best_params['stride']\n",
        "    )\n",
        "\n",
        "    # --- 6. Create the Test DataLoader ---\n",
        "    test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "    test_loader  = make_loader(\n",
        "        test_ds,\n",
        "        batch_size=final_best_params['batch_size'],\n",
        "        shuffle=False,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    # Handle empty test sets from user splits\n",
        "    if len(test_ds) == 0:\n",
        "        print(f\"  WARNING: Test set for split {split+1} is empty. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # --- 7. Initialise the Model ---\n",
        "    # Use the best architecture parameters from the grid search\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=X_test.shape[2], # num_features (3)\n",
        "        hidden_size=final_best_params['hidden_size'],\n",
        "        num_layers=final_best_params['hidden_layers'],\n",
        "        num_classes=len(label_mapping), # num_classes (6)\n",
        "        dropout_rate=final_best_params['dropout_rate'],\n",
        "        bidirectional=final_best_params['bidirectional'],\n",
        "        rnn_type=final_best_params['rnn_type']\n",
        "    ).to(device)\n",
        "\n",
        "    # --- 8. Load the model weights for this specific split and config ---\n",
        "    model_path = f\"models/{best_config_str}/split_{split}_model.pt\"\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ERROR: Model file not found at {model_path}\")\n",
        "        print(f\"  Skipping split {split+1}.\")\n",
        "        continue\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # --- 9. Run predictions on the test set ---\n",
        "    split_test_preds, split_test_targets = [], []\n",
        "    with torch.no_grad(): # Disable gradient computation for inference\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            split_test_preds.append(preds)\n",
        "            split_test_targets.append(yb.numpy())\n",
        "\n",
        "    split_test_preds = np.concatenate(split_test_preds)\n",
        "    split_test_targets = np.concatenate(split_test_targets)\n",
        "\n",
        "    # --- 10. Calculate and store metrics for this split ---\n",
        "    split_test_acc = accuracy_score(split_test_targets, split_test_preds)\n",
        "    split_test_prec = precision_score(split_test_targets, split_test_preds, average='weighted', zero_division=0)\n",
        "    split_test_rec = recall_score(split_test_targets, split_test_preds, average='weighted', zero_division=0)\n",
        "    split_test_f1 = f1_score(split_test_targets, split_test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"  Test F1 Score for Split {split+1}: {split_test_f1:.4f}\")\n",
        "\n",
        "    test_accuracies.append(split_test_acc)\n",
        "    test_precisions.append(split_test_prec)\n",
        "    test_recall_scores.append(split_test_rec)\n",
        "    test_f1_scores.append(split_test_f1)\n",
        "\n",
        "    all_test_targets.extend(split_test_targets)\n",
        "    all_test_preds.extend(split_test_preds)\n",
        "\n",
        "\n",
        "# --- 11. After the loop: Print mean metrics and plot confusion matrix ---\n",
        "print(\"\\nAverage metrics across all splits on the test set:\")\n",
        "print(f\"Mean Accuracy: {np.mean(test_accuracies):.4f} ¬± {np.std(test_accuracies):.4f}\")\n",
        "print(f\"Mean Precision: {np.mean(test_precisions):.4f} ¬± {np.std(test_precisions):.4f}\")\n",
        "print(f\"Mean Recall: {np.mean(test_recall_scores):.4f} ¬± {np.std(test_recall_scores):.4f}\")\n",
        "print(f\"Mean F1 score: {np.mean(test_f1_scores):.4f} ¬± {np.std(test_f1_scores):.4f}\")\n",
        "\n",
        "\n",
        "# Generate confusion matrix for the concatenated test sets\n",
        "cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm, annot=labels, fmt='',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Aggregated Confusion Matrix ‚Äî Test Sets Across Splits')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et_9CJzIBsRt"
      },
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "##### Connect with us:\n",
        "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n",
        "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n",
        "\n",
        "##### Contributors:\n",
        "- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n",
        "- **Alberto Archetti**: alberto.archetti@polimi.it\n",
        "- **Roberto Basla**: roberto.basla@polimi.it\n",
        "- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n",
        "\n",
        "```\n",
        "   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3bPCQoeXPjQY"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5FyEkTbAdUD"
   },
   "source": [
    "# **Challenge 1**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfehjCy896Fd"
   },
   "source": [
    "## âš™ï¸ **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x47Uv8R9Akcl",
    "outputId": "0847184b-c0c9-4e37-cb14-766c544557e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le chemin d'accï¿½s spï¿½cifiï¿½ est introuvable.\n",
      "'pkill' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exï¿½cutable ou un fichier de commandes.\n",
      "Le chemin d'accï¿½s spï¿½cifiï¿½ est introuvable.\n",
      "Un sous-rï¿½pertoire ou un fichier -p existe dï¿½jï¿½.\n",
      "Une erreur s'est produite lors du traitement deï¿½: -p.\n",
      "Un sous-rï¿½pertoire ou un fichier models existe dï¿½jï¿½.\n",
      "Une erreur s'est produite lors du traitement deï¿½: models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "# from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"models/gru_hparams.pt\"\n",
    "\n",
    "model_file = \"models/gru_model.pt\"\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRK9IPIc-ZEz"
   },
   "source": [
    "## â³ **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference dataset loading\n",
    "data_inference = torch.load(\"dataset_inference.pt\")\n",
    "\n",
    "inference_ds = TensorDataset(data_inference[\"X_inference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "    )\n",
    "inference_loader = make_loader(inference_ds, batch_size=64, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU).\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Map string name to PyTorch RNN class\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        # Create the recurrent layer\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        # Calculate input size for the final classifier\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        # LSTM returns (h_n, c_n), we only need h_n\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "\n",
    "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
    "            # Final shape: (batch_size, hidden_size * 2)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            # Take the last layer's hidden state\n",
    "            # Final shape: (batch_size, hidden_size)\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m hparams \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(hparams_file)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m RecurrentClassifier(\n\u001b[0;32m      4\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m# Pass the number of features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     rnn_type\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrnn_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_file))\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1521\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1522\u001b[0m             opened_zipfile,\n\u001b[0;32m   1523\u001b[0m             map_location,\n\u001b[0;32m   1524\u001b[0m             _weights_only_unpickler,\n\u001b[0;32m   1525\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1526\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1527\u001b[0m         )\n\u001b[0;32m   1528\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:2122\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   2121\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 2122\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   2123\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2125\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:535\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    531\u001b[0m     ):\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    533\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         )\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_load(pid))\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m    537\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:2086\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2085\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 2086\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(\n\u001b[0;32m   2087\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[0;32m   2088\u001b[0m     )\n\u001b[0;32m   2090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:2052\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   2048\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m restore_location(storage, location)\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2054\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:636\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    634\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:605\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    603\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    611\u001b[0m     )\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    613\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "\n",
    "hparams = torch.load(hparams_file)\n",
    "\n",
    "model = RecurrentClassifier(\n",
    "    input_size=hparams[\"input_size\"], # Pass the number of features\n",
    "    hidden_size=hparams[\"hidden_size\"],\n",
    "    num_layers=hparams[\"num_layers\"],\n",
    "    num_classes=hparams[\"num_classes\"],\n",
    "    dropout_rate=hparams[\"dropout_rate\"],\n",
    "    bidirectional=hparams[\"bidirectional\"],\n",
    "    rnn_type=hparams[\"rnn_type\"]\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4paoJv0k9Siw"
   },
   "source": [
    "## ðŸ”Ž **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "C0m6Gn4fkm9S",
    "outputId": "f86f00bf-90b1-4a02-f94b-a71902065e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (105760, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>time</th>\n",
       "      <th>pain_survey_1</th>\n",
       "      <th>pain_survey_2</th>\n",
       "      <th>pain_survey_3</th>\n",
       "      <th>pain_survey_4</th>\n",
       "      <th>n_legs</th>\n",
       "      <th>n_hands</th>\n",
       "      <th>n_eyes</th>\n",
       "      <th>joint_00</th>\n",
       "      <th>...</th>\n",
       "      <th>joint_21</th>\n",
       "      <th>joint_22</th>\n",
       "      <th>joint_23</th>\n",
       "      <th>joint_24</th>\n",
       "      <th>joint_25</th>\n",
       "      <th>joint_26</th>\n",
       "      <th>joint_27</th>\n",
       "      <th>joint_28</th>\n",
       "      <th>joint_29</th>\n",
       "      <th>joint_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.094705</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499558e-06</td>\n",
       "      <td>1.945042e-06</td>\n",
       "      <td>3.999558e-06</td>\n",
       "      <td>1.153299e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.026798</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>...</td>\n",
       "      <td>3.976952e-07</td>\n",
       "      <td>6.765107e-07</td>\n",
       "      <td>6.019627e-06</td>\n",
       "      <td>4.643774e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.080745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.533820e-07</td>\n",
       "      <td>1.698525e-07</td>\n",
       "      <td>1.446051e-06</td>\n",
       "      <td>2.424536e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006865e-05</td>\n",
       "      <td>5.511079e-07</td>\n",
       "      <td>1.847597e-06</td>\n",
       "      <td>5.432416e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.090185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437266e-06</td>\n",
       "      <td>1.735459e-07</td>\n",
       "      <td>1.552722e-06</td>\n",
       "      <td>5.825366e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.146031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073167e-06</td>\n",
       "      <td>1.753837e-07</td>\n",
       "      <td>2.957340e-07</td>\n",
       "      <td>6.217311e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.033101</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.025870</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074800e-06</td>\n",
       "      <td>1.772156e-07</td>\n",
       "      <td>1.976558e-06</td>\n",
       "      <td>1.576086e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.038597</td>\n",
       "      <td>...</td>\n",
       "      <td>8.829074e-07</td>\n",
       "      <td>1.790415e-07</td>\n",
       "      <td>2.210562e-06</td>\n",
       "      <td>1.485741e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>0.015732</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>0.984251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.621055e-06</td>\n",
       "      <td>1.165161e-06</td>\n",
       "      <td>3.030164e-07</td>\n",
       "      <td>5.416678e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>1.054999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609114e-06</td>\n",
       "      <td>3.959558e-06</td>\n",
       "      <td>2.017157e-06</td>\n",
       "      <td>1.154349e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>0.034006</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
       "0             0     0              2              0              2   \n",
       "1             0     1              2              2              2   \n",
       "2             0     2              2              0              2   \n",
       "3             0     3              2              2              2   \n",
       "4             0     4              2              2              2   \n",
       "5             0     5              2              0              2   \n",
       "6             0     6              2              1              2   \n",
       "7             0     7              2              2              2   \n",
       "8             0     8              2              2              0   \n",
       "9             0     9              0              2              2   \n",
       "\n",
       "   pain_survey_4 n_legs n_hands n_eyes  joint_00  ...      joint_21  \\\n",
       "0              1    two     two    two  1.094705  ...  3.499558e-06   \n",
       "1              2    two     two    two  1.135183  ...  3.976952e-07   \n",
       "2              2    two     two    two  1.080745  ...  1.533820e-07   \n",
       "3              2    two     two    two  0.938017  ...  1.006865e-05   \n",
       "4              2    two     two    two  1.090185  ...  4.437266e-06   \n",
       "5              1    two     two    two  1.146031  ...  1.073167e-06   \n",
       "6              1    two     two    two  1.025870  ...  1.074800e-06   \n",
       "7              2    two     two    two  1.038597  ...  8.829074e-07   \n",
       "8              1    two     two    two  0.984251  ...  1.621055e-06   \n",
       "9              2    two     two    two  1.054999  ...  1.609114e-06   \n",
       "\n",
       "       joint_22      joint_23      joint_24  joint_25  joint_26  joint_27  \\\n",
       "0  1.945042e-06  3.999558e-06  1.153299e-05  0.000004  0.017592  0.013508   \n",
       "1  6.765107e-07  6.019627e-06  4.643774e-08  0.000000  0.013352  0.000000   \n",
       "2  1.698525e-07  1.446051e-06  2.424536e-06  0.000003  0.016225  0.008110   \n",
       "3  5.511079e-07  1.847597e-06  5.432416e-08  0.000000  0.011832  0.007450   \n",
       "4  1.735459e-07  1.552722e-06  5.825366e-08  0.000007  0.005360  0.002532   \n",
       "5  1.753837e-07  2.957340e-07  6.217311e-08  0.000007  0.006150  0.006444   \n",
       "6  1.772156e-07  1.976558e-06  1.576086e-06  0.000005  0.006495  0.006421   \n",
       "7  1.790415e-07  2.210562e-06  1.485741e-06  0.000000  0.015998  0.005397   \n",
       "8  1.165161e-06  3.030164e-07  5.416678e-07  0.000000  0.020539  0.008517   \n",
       "9  3.959558e-06  2.017157e-06  1.154349e-06  0.000007  0.007682  0.021383   \n",
       "\n",
       "   joint_28  joint_29  joint_30  \n",
       "0  0.026798  0.027815       0.5  \n",
       "1  0.013377  0.013716       0.5  \n",
       "2  0.024097  0.023105       0.5  \n",
       "3  0.028613  0.024648       0.5  \n",
       "4  0.033026  0.025328       0.5  \n",
       "5  0.033101  0.023767       0.5  \n",
       "6  0.031804  0.019056       0.5  \n",
       "7  0.035552  0.015732       0.5  \n",
       "8  0.008635  0.015257       0.5  \n",
       "9  0.034006  0.028966       0.5  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect predictions and ground truth labels\n",
    "list_preds = []\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for xb in inference_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # Forward pass: get model predictions\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Store batch results\n",
    "        list_preds.append(preds)\n",
    "\n",
    "# Combine all batches into single arrays\n",
    "list_preds = np.concatenate(list_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et_9CJzIBsRt"
   },
   "source": [
    "#  \n",
    "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
    "\n",
    "##### Connect with us:\n",
    "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n",
    "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n",
    "\n",
    "##### Contributors:\n",
    "- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n",
    "- **Alberto Archetti**: alberto.archetti@polimi.it\n",
    "- **Roberto Basla**: roberto.basla@polimi.it\n",
    "- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n",
    "\n",
    "```\n",
    "   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3bPCQoeXPjQY"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
